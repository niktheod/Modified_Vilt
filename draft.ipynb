{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from utility import ViltImageSetProcessor\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from isvqa_data_setup import ISVQA\n",
    "from torch.utils.data import DataLoader\n",
    "from engine import max_to_one_hot\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from modified_transformers import ViltForQuestionAnswering as Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "with open(\"/home/nikostheodoridis/nuscenes/v1.0-trainval/sample_data.json\") as f:\n",
    "    set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([[[[0.5, 0.5, 2, 2, 1, 1, 0, 0]]]]).permute(3, 0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4044,  0.3408,  0.1087, -1.0835,  0.0249,  0.5760],\n",
       "        [-0.6841,  1.4932,  0.7315,  0.3696, -0.3710, -0.3811],\n",
       "        [-0.9306, -0.7599,  0.0304,  0.9442,  0.8090,  0.1756],\n",
       "        [ 0.9852,  0.1327,  0.8538,  1.1149,  1.0211, -0.2032],\n",
       "        [-0.6104, -0.2446, -0.6902,  0.4723,  0.7550,  1.8322],\n",
       "        [ 0.9440, -0.3154, -0.9049, -1.0326,  0.5432,  0.9103],\n",
       "        [ 1.2303,  1.5866,  0.1275, -1.5437, -0.0708, -0.0039],\n",
       "        [-0.1501, -1.1086, -0.3724,  0.1634, -0.4599,  0.3803]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_factor = torch.randn(8, 6)\n",
    "noise_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2725,  1.4158,  0.2247,  1.0662,  0.7006, -2.4877],\n",
       "        [-0.4855, -1.8564,  2.9844, -0.5942, -1.0811,  1.2101],\n",
       "        [-0.0240,  0.9791,  1.2967, -2.5906, -0.4644, -0.7949],\n",
       "        [-0.0980, -1.4887,  0.7977,  0.0402, -1.1820, -0.6645],\n",
       "        [-2.1476, -3.0469, -0.4683,  1.4268,  0.9250,  0.7324],\n",
       "        [ 0.8740, -0.7800,  1.0434, -0.0034,  0.2543,  0.9389],\n",
       "        [ 0.1522,  0.9004,  1.9317, -0.0290, -1.6792, -3.3612],\n",
       "        [-0.8901, -1.3162, -0.5851, -0.9978, -0.3627,  0.5429]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn_like(noise_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = torch.rand(8, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2221, 0.2914, 0.7832, 0.6062, 0.4547, 0.7412]],\n",
       "\n",
       "        [[0.9836, 0.2860, 0.2046, 0.3320, 0.6811, 0.2645]],\n",
       "\n",
       "        [[0.2581, 0.9593, 0.0395, 0.4757, 0.2544, 0.8954]],\n",
       "\n",
       "        [[0.0906, 0.6056, 0.9974, 0.6916, 0.0303, 0.3662]],\n",
       "\n",
       "        [[0.7107, 0.4780, 0.1377, 0.2741, 0.7966, 0.7660]],\n",
       "\n",
       "        [[0.3671, 0.2790, 0.9802, 0.1566, 0.4528, 0.3579]],\n",
       "\n",
       "        [[0.6882, 0.7649, 0.7570, 0.6947, 0.8298, 0.1939]],\n",
       "\n",
       "        [[0.9151, 0.1664, 0.5766, 0.6750, 0.2826, 0.6396]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7832]],\n",
       "\n",
       "        [[0.9836]],\n",
       "\n",
       "        [[0.9593]],\n",
       "\n",
       "        [[0.9974]],\n",
       "\n",
       "        [[0.7966]],\n",
       "\n",
       "        [[0.9802]],\n",
       "\n",
       "        [[0.8298]],\n",
       "\n",
       "        [[0.9151]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxs = attn_scores.max(dim=2)[0].unsqueeze(2)\n",
    "maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores.max(dim=2)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 1, 6])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(attn_scores / maxs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (attn_scores / maxs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 3)\n",
    "y = torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5050,  0.5403,  1.1741],\n",
       "        [ 1.2193, -1.1905, -1.8367]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(8, 6, 3, 352, 608)\n",
    "noise = torch.randn(8, 6, 1, 1, 1)\n",
    "noisy_image = image + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0366,  0.3002,  0.2240,  ...,  1.6317, -0.3505, -0.9450],\n",
       "         [-0.8359, -0.5737, -0.2112,  ...,  0.1756, -0.2384, -0.1210],\n",
       "         [ 1.9564, -2.2391, -0.8146,  ...,  0.1312,  1.3213,  0.6846],\n",
       "         ...,\n",
       "         [ 1.0601,  0.1381,  0.3950,  ..., -0.8237,  0.1875, -1.5251],\n",
       "         [-1.5678, -0.9031, -2.8113,  ...,  0.7849, -0.7146,  0.8592],\n",
       "         [-1.4974,  0.1687, -0.8978,  ...,  1.0746, -0.1101, -0.3491]],\n",
       "\n",
       "        [[-0.7154, -0.8828, -0.4064,  ...,  1.4242, -0.7499,  0.7787],\n",
       "         [ 0.8162, -0.1590, -0.0790,  ..., -1.4163,  0.9879,  0.6088],\n",
       "         [ 0.0808,  0.4854,  0.2819,  ..., -0.5682, -1.2184,  0.5419],\n",
       "         ...,\n",
       "         [ 0.5976, -1.0927, -1.9151,  ...,  1.0141,  0.7429,  1.4709],\n",
       "         [ 0.5953,  1.1467,  2.6890,  ...,  1.0548, -0.0602,  0.3727],\n",
       "         [-2.3645,  0.3572,  0.9312,  ...,  1.1767, -0.5813, -0.0706]],\n",
       "\n",
       "        [[-0.0658,  0.8294, -2.2025,  ..., -0.1788, -2.3541, -0.9722],\n",
       "         [ 1.6987,  1.0280, -0.7680,  ..., -1.0761,  1.8844,  0.3868],\n",
       "         [-1.0784,  2.7174, -0.1445,  ...,  0.7484, -2.2615, -0.6714],\n",
       "         ...,\n",
       "         [ 1.6617, -0.8204, -1.8125,  ..., -0.2907,  1.7766,  0.3252],\n",
       "         [-0.4107,  0.0985,  0.1784,  ..., -1.3889, -0.8850, -0.6362],\n",
       "         [-0.5076, -0.5862, -0.2473,  ...,  1.0262,  0.1952, -0.7600]]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[7, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1038]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise[7, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1404,  0.1964,  0.1202,  ...,  1.5279, -0.4543, -1.0488],\n",
       "         [-0.9397, -0.6775, -0.3150,  ...,  0.0718, -0.3421, -0.2248],\n",
       "         [ 1.8526, -2.3429, -0.9184,  ...,  0.0274,  1.2176,  0.5808],\n",
       "         ...,\n",
       "         [ 0.9563,  0.0343,  0.2912,  ..., -0.9274,  0.0837, -1.6288],\n",
       "         [-1.6716, -1.0069, -2.9151,  ...,  0.6811, -0.8184,  0.7554],\n",
       "         [-1.6012,  0.0649, -1.0015,  ...,  0.9708, -0.2139, -0.4528]],\n",
       "\n",
       "        [[-0.8192, -0.9866, -0.5102,  ...,  1.3205, -0.8537,  0.6749],\n",
       "         [ 0.7125, -0.2628, -0.1828,  ..., -1.5201,  0.8841,  0.5050],\n",
       "         [-0.0229,  0.3816,  0.1781,  ..., -0.6719, -1.3221,  0.4381],\n",
       "         ...,\n",
       "         [ 0.4938, -1.1965, -2.0189,  ...,  0.9103,  0.6391,  1.3672],\n",
       "         [ 0.4915,  1.0430,  2.5853,  ...,  0.9510, -0.1640,  0.2690],\n",
       "         [-2.4682,  0.2534,  0.8274,  ...,  1.0730, -0.6851, -0.1744]],\n",
       "\n",
       "        [[-0.1696,  0.7257, -2.3063,  ..., -0.2826, -2.4578, -1.0760],\n",
       "         [ 1.5950,  0.9242, -0.8718,  ..., -1.1799,  1.7807,  0.2830],\n",
       "         [-1.1821,  2.6137, -0.2482,  ...,  0.6446, -2.3652, -0.7752],\n",
       "         ...,\n",
       "         [ 1.5579, -0.9242, -1.9163,  ..., -0.3944,  1.6728,  0.2214],\n",
       "         [-0.5144, -0.0052,  0.0746,  ..., -1.4927, -0.9888, -0.7400],\n",
       "         [-0.6114, -0.6899, -0.3511,  ...,  0.9224,  0.0914, -0.8638]]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_image[7, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4297,  0.7265,  1.7333],\n",
       "        [ 1.7115, -0.4212, -0.6956]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6467, 0.3925, 2.0350],\n",
       "        [2.0867, 0.5014, 1.2775]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7164, 0.6280, 0.0000, 0.2260, 0.4195, 0.0537],\n",
       "        [0.0000, 0.7092, 0.7920, 0.6624, 0.3075, 0.7311],\n",
       "        [0.7310, 0.0000, 0.9588, 0.5041, 0.7348, 0.0667],\n",
       "        [0.9092, 0.3929, 0.0000, 0.3066, 0.9697, 0.6328],\n",
       "        [0.1079, 0.4000, 0.8271, 0.6559, 0.0000, 0.0384],\n",
       "        [0.6255, 0.7154, 0.0000, 0.8403, 0.5381, 0.6349],\n",
       "        [0.1707, 0.0783, 0.0878, 0.1628, 0.0000, 0.7663],\n",
       "        [0.0000, 0.8181, 0.3700, 0.2624, 0.6912, 0.3011]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 1, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.randn(8, 6, 3, 352, 608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_image = weights * image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 3, 352, 608])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2311,  0.2379,  0.9339,  ..., -1.0574,  0.9730,  0.1771],\n",
       "         [ 0.5407,  0.4366,  0.5054,  ...,  0.6561,  1.0277,  1.1712],\n",
       "         [ 0.2865, -0.8768,  1.0330,  ...,  0.8223, -0.2839,  0.5287],\n",
       "         ...,\n",
       "         [ 0.7281,  1.2110, -0.7459,  ..., -1.3635,  0.0546, -0.2609],\n",
       "         [-0.8802,  1.7907,  0.7237,  ..., -0.3063,  0.2031, -1.7614],\n",
       "         [-1.8903,  0.6733,  1.3758,  ..., -0.6519,  0.4261,  1.1572]],\n",
       "\n",
       "        [[ 0.4723,  0.9483, -0.8223,  ...,  0.8321, -2.1512,  1.9406],\n",
       "         [ 0.7125, -0.2474, -0.4329,  ...,  0.3571, -0.0951, -1.0156],\n",
       "         [-0.7591, -1.4258,  0.8662,  ...,  0.5441,  0.4617,  0.1125],\n",
       "         ...,\n",
       "         [ 0.2302, -0.0024, -0.5610,  ...,  0.3369,  1.3106, -0.3004],\n",
       "         [ 1.2285, -1.4839,  1.1372,  ...,  0.7506,  1.4058, -0.7462],\n",
       "         [ 0.2507, -0.6956, -1.0991,  ...,  0.9928,  0.3242,  1.5292]],\n",
       "\n",
       "        [[ 0.4740,  0.0262, -0.8766,  ..., -0.7047, -0.4071,  0.3702],\n",
       "         [ 0.4548, -0.1914, -1.0548,  ...,  0.0812,  0.3173,  0.0191],\n",
       "         [-2.1856,  0.5872, -0.6281,  ...,  0.2341,  1.0345, -1.6361],\n",
       "         ...,\n",
       "         [-0.0876,  0.9588, -0.2555,  ..., -0.1848,  0.3114,  0.3491],\n",
       "         [-1.2949,  0.4217,  0.4708,  ...,  0.3250, -2.0592,  0.4128],\n",
       "         [ 0.5027,  0.4020,  1.1131,  ...,  0.1927, -0.8620, -1.7405]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9333]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2157,  0.2220,  0.8717,  ..., -0.9869,  0.9081,  0.1653],\n",
       "         [ 0.5047,  0.4075,  0.4717,  ...,  0.6124,  0.9592,  1.0931],\n",
       "         [ 0.2674, -0.8183,  0.9641,  ...,  0.7675, -0.2650,  0.4934],\n",
       "         ...,\n",
       "         [ 0.6796,  1.1303, -0.6962,  ..., -1.2726,  0.0509, -0.2435],\n",
       "         [-0.8215,  1.6713,  0.6755,  ..., -0.2859,  0.1896, -1.6439],\n",
       "         [-1.7642,  0.6284,  1.2840,  ..., -0.6085,  0.3977,  1.0801]],\n",
       "\n",
       "        [[ 0.4408,  0.8851, -0.7675,  ...,  0.7766, -2.0077,  1.8112],\n",
       "         [ 0.6650, -0.2309, -0.4041,  ...,  0.3333, -0.0888, -0.9479],\n",
       "         [-0.7085, -1.3307,  0.8085,  ...,  0.5079,  0.4309,  0.1050],\n",
       "         ...,\n",
       "         [ 0.2148, -0.0022, -0.5236,  ...,  0.3145,  1.2232, -0.2804],\n",
       "         [ 1.1465, -1.3849,  1.0613,  ...,  0.7006,  1.3121, -0.6965],\n",
       "         [ 0.2340, -0.6493, -1.0258,  ...,  0.9266,  0.3026,  1.4273]],\n",
       "\n",
       "        [[ 0.4424,  0.0245, -0.8181,  ..., -0.6577, -0.3799,  0.3455],\n",
       "         [ 0.4244, -0.1786, -0.9845,  ...,  0.0758,  0.2961,  0.0179],\n",
       "         [-2.0399,  0.5481, -0.5862,  ...,  0.2185,  0.9655, -1.5270],\n",
       "         ...,\n",
       "         [-0.0817,  0.8949, -0.2385,  ..., -0.1724,  0.2906,  0.3259],\n",
       "         [-1.2086,  0.3936,  0.4394,  ...,  0.3033, -1.9219,  0.3853],\n",
       "         [ 0.4691,  0.3752,  1.0389,  ...,  0.1798, -0.8046, -1.6244]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_image[2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 2, 2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikostheodoridis/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Baseline.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViltForQuestionAnswering(\n",
      "  (vilt): ViltModel(\n",
      "    (embeddings): ViltEmbeddings(\n",
      "      (text_embeddings): TextEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768)\n",
      "        (position_embeddings): Embedding(40, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (patch_embeddings): ViltPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "      )\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViltEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViltLayer(\n",
      "          (attention): ViltAttention(\n",
      "            (attention): ViltSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): ViltSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViltIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViltOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (pooler): ViltPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
      "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Linear(in_features=1536, out_features=3129, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 6, 3, 352, 608)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 3, 352, 608])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.3835)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaClassificationHead(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=1, bias=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikostheodoridis/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MultiviewViltForQuestionAnsweringBaseline(6, 210, 768, True, True, True, 429).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vilt.modeling_vilt import ViltEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.11'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "  (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (2): GELU(approximate='none')\n",
       "  (3): Linear(in_features=1536, out_features=429, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "isvqa = ISVQA(\"/home/nikostheodoridis/isvqa/train_set.json\",\n",
    "              \"/home/nikostheodoridis/nuscenes/samples\",\n",
    "              \"/home/nikostheodoridis/isvqa/answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1., device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isvqa[0][0][\"pixel_values\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(isvqa, 4, shuffle=False)\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 429])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = torch.randn(4, 1, 768)\n",
    "images = torch.randn(4, 6, 768)\n",
    "\n",
    "attn_scores = torch.randn(4, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikostheodoridis/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::copy_        46.67%      69.913ms        71.95%     107.799ms     128.638us     110.855ms        68.45%     110.855ms     132.285us           838  \n",
      "                                   aten::to         0.57%     857.000us        29.40%      44.043ms     205.808us       1.066ms         0.66%      44.273ms     206.883us           214  \n",
      "                             aten::_to_copy         1.20%       1.795ms        28.79%      43.127ms     201.528us       1.437ms         0.89%      43.207ms     201.902us           214  \n",
      "                               aten::detach        15.07%      22.581ms        17.18%      25.740ms       4.414us      25.315ms        15.63%      36.165ms       6.202us          5831  \n",
      "                                     detach         0.59%     889.000us         0.59%     889.000us       0.152us      10.857ms         6.70%      10.857ms       1.862us          5831  \n",
      "                                aten::empty         1.49%       2.236ms         1.49%       2.236ms       1.762us       5.157ms         3.18%       5.157ms       4.064us          1269  \n",
      "                        aten::empty_strided         0.42%     635.000us         1.67%       2.504ms      11.701us       3.675ms         2.27%       3.675ms      17.173us           214  \n",
      "                                aten::fill_         0.83%       1.243ms         0.83%       1.243ms      15.159us       1.340ms         0.83%       1.340ms      16.341us            82  \n",
      "                                 aten::set_         0.06%      90.000us         0.06%      90.000us       0.141us       1.244ms         0.77%       1.244ms       1.947us           639  \n",
      "                                aten::zero_         0.03%      46.000us         0.59%     884.000us      10.045us     289.000us         0.18%     902.000us      10.250us            88  \n",
      "                                aten::zeros         0.05%      76.000us         0.64%     956.000us     106.222us      57.000us         0.04%     711.000us      79.000us             9  \n",
      "    aten::_has_compatible_shallow_copy_type         0.00%       2.000us         0.00%       2.000us       0.005us     453.000us         0.28%     453.000us       1.068us           424  \n",
      "                               aten::arange         0.06%      83.000us         0.09%     140.000us      23.333us     179.000us         0.11%     218.000us      36.333us             6  \n",
      "                               aten::expand         0.03%      41.000us         0.03%      47.000us      15.667us      20.000us         0.01%      27.000us       9.000us             3  \n",
      "                            cudaEventRecord         6.30%       9.441ms         6.30%       9.441ms       0.301us       0.000us         0.00%      12.000us       0.000us         31336  \n",
      "                              aten::resize_         0.00%       3.000us         0.00%       3.000us       1.000us       7.000us         0.00%       7.000us       2.333us             3  \n",
      "                           aten::as_strided         0.00%       6.000us         0.00%       6.000us       2.000us       7.000us         0.00%       7.000us       2.333us             3  \n",
      "           cudaDeviceGetStreamPriorityRange         0.00%       1.000us         0.00%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                         cudaGetDeviceCount         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                      cudaStreamIsCapturing         0.00%       5.000us         0.00%       5.000us       0.200us       0.000us         0.00%       0.000us       0.000us            25  \n",
      "                 cudaGetDeviceProperties_v2         0.03%      51.000us         0.03%      51.000us      51.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                 cudaMalloc         1.25%       1.867ms         1.25%       1.867ms      84.864us       0.000us         0.00%       0.000us       0.000us            22  \n",
      "                            cudaMemcpyAsync        21.63%      32.411ms        21.63%      32.411ms     151.453us       0.000us         0.00%       0.000us       0.000us           214  \n",
      "                      cudaStreamSynchronize         3.70%       5.541ms         3.70%       5.541ms      25.893us       0.000us         0.00%       0.000us       0.000us           214  \n",
      "                      cudaDeviceSynchronize         0.00%       3.000us         0.00%       3.000us       3.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 149.816ms\n",
      "Self CUDA time total: 161.958ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    model = MultiviewViltForQuestionAnsweringBaseline(6, 210, 768, True, True, False).to(\"cuda\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(isvqa, 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.Size' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1913, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = torch.randn(1, 768)\n",
    "images = torch.randn(6, 768)\n",
    "\n",
    "img_attn = nn.MultiheadAttention(768, 12)\n",
    "\n",
    "_, attn_scores = img_attn(questions, images, images)\n",
    "\n",
    "attn_scores[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model, loader, acc_fn, answ_len):\n",
    "    \"\"\"\n",
    "    A function that validates the model by going through all the mini-batches in the validation dataloader once.\n",
    "    \"\"\"\n",
    "    print(\"\\tValidating...\")\n",
    "    model.eval()\n",
    "    losses = []  # to save the loss of each mini-batch in order to take their average at the end\n",
    "    accuracies = []  # to save the accuracy of each mini-batch in order to take their average at the end\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (X, y) in enumerate(loader):\n",
    "            outputs = model(**X, labels=y)\n",
    "            loss = outputs.loss\n",
    "            pred = max_to_one_hot(outputs.logits)\n",
    "            acc = acc_fn(pred, y, answ_len)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(acc)\n",
    "\n",
    "    avg_loss = sum(losses) / len(loader)\n",
    "    avg_acc = sum(accuracies) / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikostheodoridis/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MultiviewViltForQuestionAnswering(6, 210, 768, True, False, False).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.classifier = nn.Sequential(\n",
    "        nn.Linear(768, 1536),\n",
    "        nn.LayerNorm(1536),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(1536, 429)\n",
    "    ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.load_state_dict(torch.load(\"/home/nikostheodoridis/Trained Models/2024-07-08 00:07:49/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p1, p2 in zip(model.parameters(), trained_model.parameters()):\n",
    "#     assert torch.equal(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = ISVQA(qa_path=\"/home/nikostheodoridis/isvqa/val_set.json\",\n",
    "                nuscenes_path=\"/home/nikostheodoridis/nuscenes/samples\",\n",
    "                answers_path=\"/home/nikostheodoridis/isvqa/answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_set, batch_size=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = []\n",
    "# untrained_predictions = []\n",
    "# trained_predictions = []\n",
    "\n",
    "# model.eval()\n",
    "# trained_model.eval()\n",
    "# for i in range(2576):\n",
    "#     inputs, target = val_set[i]\n",
    "\n",
    "#     targets.append(target)\n",
    "\n",
    "#     with torch.inference_mode():\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions: torch.Tensor, targets: torch.Tensor, answers_len: int) -> float:\n",
    "    cnt = torch.eq(torch.eq(predictions, targets).sum(dim=1), answers_len).sum()\n",
    "    return cnt.item() / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidating...\n"
     ]
    }
   ],
   "source": [
    "untrained_loss, untrained_acc = val_step(model, val_loader, accuracy, 429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidating...\n"
     ]
    }
   ],
   "source": [
    "trained_loss, trained_acc = val_step(trained_model, val_loader, accuracy, 429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303.8099614342978\n",
      "0.0011627906976744186\n"
     ]
    }
   ],
   "source": [
    "print(untrained_loss)\n",
    "print(untrained_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5099486532945967\n",
      "0.6003875968992246\n"
     ]
    }
   ],
   "source": [
    "print(trained_loss)\n",
    "print(trained_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nikostheodoridis/isvqa/answers_counter.json\") as f:\n",
    "    answers_cnt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'yes': 13564,\n",
       "         'no': 3734,\n",
       "         'one': 3663,\n",
       "         'white': 2893,\n",
       "         'two': 2544,\n",
       "         'red': 1205,\n",
       "         'black': 1046,\n",
       "         'blue': 1025,\n",
       "         'three': 986,\n",
       "         'green': 968,\n",
       "         'yellow': 930,\n",
       "         'orange': 782,\n",
       "         'four': 529,\n",
       "         'night': 464,\n",
       "         'rainy': 434,\n",
       "         'gray': 365,\n",
       "         'black and white': 345,\n",
       "         'silver': 287,\n",
       "         'zero': 254,\n",
       "         'five': 218,\n",
       "         'orange and white': 178,\n",
       "         'six': 156,\n",
       "         'left': 151,\n",
       "         'none': 147,\n",
       "         'ahead': 147,\n",
       "         'right': 141,\n",
       "         'fedex': 136,\n",
       "         'brown': 134,\n",
       "         'cloudy': 129,\n",
       "         'slow': 119,\n",
       "         'ups': 114,\n",
       "         'bus': 112,\n",
       "         'raining': 111,\n",
       "         'wet': 111,\n",
       "         'sunny': 107,\n",
       "         'ryder': 95,\n",
       "         'urban': 93,\n",
       "         'twenty-three': 92,\n",
       "         'stop': 89,\n",
       "         'day': 77,\n",
       "         'hump': 69,\n",
       "         'brick': 69,\n",
       "         'red and white': 57,\n",
       "         'twenty-five': 55,\n",
       "         'bridge': 55,\n",
       "         'rectangle': 54,\n",
       "         'trees': 53,\n",
       "         'truck': 52,\n",
       "         'city': 51,\n",
       "         'toyota': 50,\n",
       "         'nineteen': 48,\n",
       "         'on': 46,\n",
       "         'rain': 40,\n",
       "         'seven': 39,\n",
       "         'forty': 37,\n",
       "         'french': 36,\n",
       "         'black and yellow': 36,\n",
       "         'innosparks': 35,\n",
       "         'square': 34,\n",
       "         'ford': 34,\n",
       "         'blue and green': 34,\n",
       "         'umbrella': 33,\n",
       "         'night time': 33,\n",
       "         'metal': 32,\n",
       "         'arrow': 32,\n",
       "         'van': 31,\n",
       "         'motorcycle': 29,\n",
       "         'parked': 29,\n",
       "         'fence': 29,\n",
       "         'construction': 28,\n",
       "         'nighttime': 28,\n",
       "         'p': 28,\n",
       "         'eversource energy': 28,\n",
       "         'green and blue': 28,\n",
       "         'nuss': 27,\n",
       "         'car': 27,\n",
       "         'crosswalk': 27,\n",
       "         'boston': 26,\n",
       "         'dark': 26,\n",
       "         'one way': 26,\n",
       "         'cars': 25,\n",
       "         'tree': 25,\n",
       "         'yankee': 25,\n",
       "         'do not enter': 25,\n",
       "         'water': 25,\n",
       "         'building': 25,\n",
       "         'twelve': 24,\n",
       "         'twenty-one': 24,\n",
       "         'pink': 24,\n",
       "         'suv': 23,\n",
       "         'starbucks': 23,\n",
       "         'north coast': 23,\n",
       "         'bicycle': 23,\n",
       "         'blue and white': 23,\n",
       "         'john nagle': 23,\n",
       "         'hump ahead': 22,\n",
       "         'eight': 22,\n",
       "         'behind': 22,\n",
       "         'bike': 21,\n",
       "         'xfinity': 21,\n",
       "         'sidewalk': 21,\n",
       "         'm': 20,\n",
       "         'harbor place': 20,\n",
       "         'usps': 20,\n",
       "         'both': 20,\n",
       "         'round': 20,\n",
       "         'parking lot': 20,\n",
       "         'white and orange': 19,\n",
       "         'orange and gray': 19,\n",
       "         'only': 19,\n",
       "         'stop sign': 19,\n",
       "         'ninety': 19,\n",
       "         'chain link': 19,\n",
       "         'penske': 19,\n",
       "         'fusionopolis two': 19,\n",
       "         'jeep': 18,\n",
       "         'holland ave': 18,\n",
       "         'gemalto': 18,\n",
       "         'white and yellow': 18,\n",
       "         'concrete': 18,\n",
       "         'hood': 18,\n",
       "         'circle': 17,\n",
       "         'gray and orange': 17,\n",
       "         'solid': 17,\n",
       "         'river': 17,\n",
       "         'synthesis': 17,\n",
       "         'backpack': 17,\n",
       "         'nine': 16,\n",
       "         'row thirty-four': 16,\n",
       "         'grass': 16,\n",
       "         'sportello': 16,\n",
       "         'yellow and black': 16,\n",
       "         'starbucks coffee': 16,\n",
       "         'dhl': 16,\n",
       "         'residential': 16,\n",
       "         'v': 16,\n",
       "         'caution': 15,\n",
       "         'one north gateway': 15,\n",
       "         'back right': 15,\n",
       "         'dog': 15,\n",
       "         'clouds': 15,\n",
       "         'crane': 15,\n",
       "         'nucleos': 15,\n",
       "         'bus stop': 14,\n",
       "         'closed': 14,\n",
       "         'nine hundred and seventy': 14,\n",
       "         'fire hydrant': 14,\n",
       "         'one north link': 14,\n",
       "         'open': 14,\n",
       "         'harvey': 13,\n",
       "         'glass': 13,\n",
       "         'bricks': 13,\n",
       "         'worldwide services': 13,\n",
       "         'speedpost': 12,\n",
       "         'walking': 12,\n",
       "         'road': 12,\n",
       "         'wheelchair': 12,\n",
       "         'museum': 12,\n",
       "         'seventy-five': 12,\n",
       "         'more': 12,\n",
       "         'innovationsquareboston': 12,\n",
       "         'yankee lobster': 12,\n",
       "         'sedan': 11,\n",
       "         'rectangular': 11,\n",
       "         'yellow and white': 11,\n",
       "         'x ing': 11,\n",
       "         'harpoon brewery': 11,\n",
       "         'evening': 11,\n",
       "         'canteen': 11,\n",
       "         'business link': 11,\n",
       "         'down': 11,\n",
       "         'ladder': 10,\n",
       "         'front': 10,\n",
       "         'ten': 10,\n",
       "         'wood': 10,\n",
       "         'msc': 10,\n",
       "         'overcast': 10,\n",
       "         'barbed wire': 10,\n",
       "         'two way': 10,\n",
       "         'shorts': 10,\n",
       "         'woman': 10,\n",
       "         'cat': 9,\n",
       "         'three way': 9,\n",
       "         'sysco': 9,\n",
       "         'centros': 9,\n",
       "         'light': 9,\n",
       "         'parking garage': 9,\n",
       "         'x': 9,\n",
       "         'empty': 9,\n",
       "         'works ahead': 9,\n",
       "         'ninety-three': 9,\n",
       "         'mercedes': 9,\n",
       "         'diamond': 9,\n",
       "         'biopolis dr': 9,\n",
       "         'front left': 9,\n",
       "         'cones': 9,\n",
       "         'commercial': 9,\n",
       "         'dunkin donuts': 9,\n",
       "         'seafood': 9,\n",
       "         'suffolk': 9,\n",
       "         's': 9,\n",
       "         'skanska': 9,\n",
       "         '': 8,\n",
       "         'mass bay credit union': 8,\n",
       "         'one hundred and fifty-one': 8,\n",
       "         'triangle': 8,\n",
       "         'four hundred and fifty-one': 8,\n",
       "         'five eleven': 8,\n",
       "         'macco energy': 8,\n",
       "         'mail truck': 8,\n",
       "         'back': 8,\n",
       "         'honda': 8,\n",
       "         'helmet': 8,\n",
       "         'nus': 8,\n",
       "         'double': 8,\n",
       "         'industrial': 8,\n",
       "         'squares': 8,\n",
       "         'bag': 8,\n",
       "         'taxi': 8,\n",
       "         'stopped': 7,\n",
       "         'four way': 7,\n",
       "         'whiskey priest': 7,\n",
       "         'traffic lights': 7,\n",
       "         'go': 7,\n",
       "         'morning': 7,\n",
       "         'usa': 7,\n",
       "         'reduce speed now': 7,\n",
       "         'dry': 7,\n",
       "         'green and white': 7,\n",
       "         'gold': 7,\n",
       "         'bicycles': 7,\n",
       "         'we deliver for you': 7,\n",
       "         'eleven': 7,\n",
       "         'bench': 7,\n",
       "         'innovation and design': 7,\n",
       "         'legal': 7,\n",
       "         'la casa de pedro': 7,\n",
       "         'sandcrawler': 7,\n",
       "         'traffic cones': 7,\n",
       "         'fort point market': 7,\n",
       "         'exit only': 7,\n",
       "         'red and gray': 7,\n",
       "         'vpne': 7,\n",
       "         'autodesk': 6,\n",
       "         'scooter': 6,\n",
       "         'rounded': 6,\n",
       "         'brown and white': 6,\n",
       "         'stars ave': 6,\n",
       "         'female': 6,\n",
       "         'enterprise': 6,\n",
       "         'twenty-two': 6,\n",
       "         'orange and black': 6,\n",
       "         'forward': 6,\n",
       "         'white and black': 6,\n",
       "         'male': 6,\n",
       "         \"don't walk\": 6,\n",
       "         'bmw': 6,\n",
       "         'red light': 6,\n",
       "         'blue harvest': 6,\n",
       "         'plants': 6,\n",
       "         'to right': 6,\n",
       "         'dashed': 6,\n",
       "         'red and blue': 6,\n",
       "         'available liftgate service': 6,\n",
       "         'www': 6,\n",
       "         'away': 6,\n",
       "         'boston freight terminals': 6,\n",
       "         'parking': 6,\n",
       "         'flett': 6,\n",
       "         'rosa mexicano': 5,\n",
       "         'on sidewalk': 5,\n",
       "         'back left': 5,\n",
       "         'cone': 5,\n",
       "         'isuzu': 5,\n",
       "         'windows': 5,\n",
       "         'sbwtc': 5,\n",
       "         'lights': 5,\n",
       "         'american': 5,\n",
       "         'portsdown rd': 5,\n",
       "         'e': 5,\n",
       "         'man': 5,\n",
       "         'true': 5,\n",
       "         'dark blue': 5,\n",
       "         'flowers': 5,\n",
       "         'arch': 5,\n",
       "         'wagamama': 5,\n",
       "         'fifteen': 5,\n",
       "         'curved': 5,\n",
       "         'kent ridge drive': 5,\n",
       "         'd': 5,\n",
       "         '4x4': 5,\n",
       "         'clear': 5,\n",
       "         'railroad crossing': 5,\n",
       "         'moving': 5,\n",
       "         'ayer rajah': 5,\n",
       "         'sitting': 5,\n",
       "         'phone': 5,\n",
       "         'salmon': 5,\n",
       "         'parking meter': 5,\n",
       "         'person': 5,\n",
       "         'straight': 5,\n",
       "         'xpo': 5,\n",
       "         'supercamp': 5,\n",
       "         'minivan': 5,\n",
       "         'tcoms': 5,\n",
       "         'factory outlet': 5,\n",
       "         'english': 5,\n",
       "         'palm tree': 5,\n",
       "         'mediapolis': 5,\n",
       "         'sign': 5,\n",
       "         'audi': 5,\n",
       "         'thirty-six': 5,\n",
       "         'seaport world trade center': 5,\n",
       "         'mail': 5,\n",
       "         'all': 5,\n",
       "         'construction ahead': 4,\n",
       "         'suburban': 4,\n",
       "         'mitsubishi': 4,\n",
       "         'bush': 4,\n",
       "         'sunglasses': 4,\n",
       "         'yang ming': 4,\n",
       "         'blue and yellow': 4,\n",
       "         'mitsubishi electric': 4,\n",
       "         'double decker bus': 4,\n",
       "         'towards': 4,\n",
       "         'up': 4,\n",
       "         'legal harborside': 4,\n",
       "         'lobster': 4,\n",
       "         'fios': 4,\n",
       "         'radius bank': 4,\n",
       "         'standing': 4,\n",
       "         'arrows': 4,\n",
       "         'triangles': 4,\n",
       "         'lexus': 4,\n",
       "         'green and red': 4,\n",
       "         'false': 4,\n",
       "         'nothing': 4,\n",
       "         'pedestrians': 4,\n",
       "         'gateway': 4,\n",
       "         'cranes': 4,\n",
       "         'paul w': 4,\n",
       "         'harpoon': 4,\n",
       "         'smoke shop': 4,\n",
       "         'red and green': 4,\n",
       "         'donuts': 4,\n",
       "         'green and orange': 4,\n",
       "         'jeans': 4,\n",
       "         'blue and orange': 4,\n",
       "         'singapore science park': 4,\n",
       "         'ship': 4,\n",
       "         'alumni house': 4,\n",
       "         'on road': 4,\n",
       "         'stroller': 4,\n",
       "         'exit': 4,\n",
       "         'bushes': 4,\n",
       "         'detour': 4,\n",
       "         'blue harvest fisheries': 4,\n",
       "         'partly cloudy': 4,\n",
       "         'w': 4,\n",
       "         'bank of america': 4,\n",
       "         'white car': 4,\n",
       "         'cell phone': 4,\n",
       "         'innovation': 4,\n",
       "         'mountain': 4,\n",
       "         'excavator': 3,\n",
       "         'yield': 3,\n",
       "         'three hundred': 3,\n",
       "         'thirty-five': 3,\n",
       "         'mazda': 3,\n",
       "         'dusk': 3,\n",
       "         'sun': 3,\n",
       "         'beige': 3,\n",
       "         'crossing street': 3,\n",
       "         'singapore': 3,\n",
       "         'outside': 3,\n",
       "         'bad': 3,\n",
       "         'site access': 3,\n",
       "         'dead end': 3,\n",
       "         'geico': 3,\n",
       "         'four hundred and eleven': 3,\n",
       "         'stairs': 3,\n",
       "         'low': 3,\n",
       "         'h': 3,\n",
       "         'sahara': 3,\n",
       "         'chevy': 3,\n",
       "         'buildings': 3,\n",
       "         'sbs transit': 3,\n",
       "         'six hundred and seventeen five hundred and twenty-three eight thousand': 3,\n",
       "         'fusionopolis': 3,\n",
       "         'traffic light': 3,\n",
       "         'daylight': 3,\n",
       "         'self storage': 3,\n",
       "         'palm trees': 3,\n",
       "         'american flag': 3,\n",
       "         'short': 3,\n",
       "         'seventy-nine': 3,\n",
       "         'biopolis rd': 3,\n",
       "         'street': 3,\n",
       "         'mandm': 3,\n",
       "         'blue dragon': 3,\n",
       "         'humps': 3,\n",
       "         'twenty': 3,\n",
       "         'white and red': 3,\n",
       "         't': 3,\n",
       "         'mailbox': 3,\n",
       "         'peapod': 3,\n",
       "         'congress st': 3,\n",
       "         'future starts here': 3,\n",
       "         'park': 3,\n",
       "         'large': 3,\n",
       "         'construction workers': 3,\n",
       "         'mirror': 3,\n",
       "         'hat': 3,\n",
       "         'twenty-eight': 3,\n",
       "         'foley': 3,\n",
       "         '02148m0003': 3,\n",
       "         'tan': 3,\n",
       "         'synapse': 3,\n",
       "         'business': 3,\n",
       "         'green and yellow': 3,\n",
       "         'pickup truck': 3,\n",
       "         'pandg': 3,\n",
       "         'dark gray': 3,\n",
       "         'intersection': 3,\n",
       "         'public parking': 3,\n",
       "         'for lease': 3,\n",
       "         'striped': 3,\n",
       "         'light blue': 3,\n",
       "         'trucks': 3})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(answers_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Start\n",
    "import json\n",
    "import random\n",
    "train_path = \"/home/nikostheodoridis/nuscenes-qa/train_set.json\"\n",
    "\n",
    "val_path = \"/home/nikostheodoridis/nuscenes-qa/val_set.json\"\n",
    "\n",
    "test_path = \"/home/nikostheodoridis/nuscenes-qa/test_set.json\"\n",
    "with open(train_path) as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(val_path) as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open(test_path) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "for data in train_data:\n",
    "    if data in test_data:\n",
    "        print(\"False\")\n",
    "        break\n",
    "else:\n",
    "    print(\"True\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
