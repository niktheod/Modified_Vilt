{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from utility import ViltImageSetProcessor\n",
    "from transformers import ViltImageProcessor, BertTokenizer, ViltModel, ViltConfig, ViltForQuestionAnswering\n",
    "from models import MultiviewViltModel, MultiviewViltForQuestionAnsweringBaseline\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from isvqa_data_setup import ISVQA\n",
    "from torch.utils.data import DataLoader\n",
    "from engine import max_to_one_hot\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "with open(\"/home/nikostheodoridis/nuscenes/v1.0-trainval/sample_data.json\") as f:\n",
    "    set = json.load(f)\n",
    "\n",
    "print(\"djsdjsjsdksj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikostheodoridis/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MultiviewViltForQuestionAnsweringBaseline(6, 210, 768, True, True, True, 429).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+------------+\n",
      "|                              Modules                               | Parameters |\n",
      "+--------------------------------------------------------------------+------------+\n",
      "|          encoder.layer.0.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.0.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.0.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.0.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.0.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.0.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.0.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.0.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.0.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.0.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.0.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.0.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.0.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.0.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.0.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.0.layernorm_after.bias                |    768     |\n",
      "|          encoder.layer.1.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.1.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.1.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.1.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.1.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.1.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.1.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.1.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.1.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.1.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.1.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.1.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.1.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.1.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.1.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.1.layernorm_after.bias                |    768     |\n",
      "|          encoder.layer.2.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.2.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.2.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.2.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.2.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.2.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.2.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.2.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.2.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.2.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.2.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.2.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.2.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.2.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.2.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.2.layernorm_after.bias                |    768     |\n",
      "|          encoder.layer.3.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.3.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.3.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.3.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.3.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.3.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.3.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.3.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.3.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.3.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.3.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.3.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.3.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.3.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.3.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.3.layernorm_after.bias                |    768     |\n",
      "|          encoder.layer.4.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.4.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.4.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.4.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.4.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.4.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.4.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.4.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.4.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.4.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.4.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.4.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.4.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.4.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.4.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.4.layernorm_after.bias                |    768     |\n",
      "|          encoder.layer.5.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.5.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.5.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.5.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.5.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.5.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.5.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.5.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.5.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.5.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.5.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.5.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.5.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.5.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.5.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.5.layernorm_after.bias                |    768     |\n",
      "|          encoder.layer.6.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.6.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.6.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.6.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.6.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.6.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.6.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.6.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.6.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.6.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.6.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.6.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.6.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.6.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.6.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.6.layernorm_after.bias                |    768     |\n",
      "|          encoder.layer.7.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.7.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.7.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.7.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.7.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.7.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.7.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.7.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.7.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.7.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.7.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.7.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.7.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.7.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.7.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.7.layernorm_after.bias                |    768     |\n",
      "|          encoder.layer.8.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.8.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.8.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.8.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.8.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.8.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.8.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.8.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.8.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.8.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.8.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.8.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.8.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.8.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.8.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.8.layernorm_after.bias                |    768     |\n",
      "|          encoder.layer.9.attention.attention.query.weight          |   589824   |\n",
      "|           encoder.layer.9.attention.attention.query.bias           |    768     |\n",
      "|           encoder.layer.9.attention.attention.key.weight           |   589824   |\n",
      "|            encoder.layer.9.attention.attention.key.bias            |    768     |\n",
      "|          encoder.layer.9.attention.attention.value.weight          |   589824   |\n",
      "|           encoder.layer.9.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.9.attention.output.dense.weight            |   589824   |\n",
      "|            encoder.layer.9.attention.output.dense.bias             |    768     |\n",
      "|             encoder.layer.9.intermediate.dense.weight              |  2359296   |\n",
      "|              encoder.layer.9.intermediate.dense.bias               |    3072    |\n",
      "|                encoder.layer.9.output.dense.weight                 |  2359296   |\n",
      "|                 encoder.layer.9.output.dense.bias                  |    768     |\n",
      "|              encoder.layer.9.layernorm_before.weight               |    768     |\n",
      "|               encoder.layer.9.layernorm_before.bias                |    768     |\n",
      "|               encoder.layer.9.layernorm_after.weight               |    768     |\n",
      "|                encoder.layer.9.layernorm_after.bias                |    768     |\n",
      "|         encoder.layer.10.attention.attention.query.weight          |   589824   |\n",
      "|          encoder.layer.10.attention.attention.query.bias           |    768     |\n",
      "|          encoder.layer.10.attention.attention.key.weight           |   589824   |\n",
      "|           encoder.layer.10.attention.attention.key.bias            |    768     |\n",
      "|         encoder.layer.10.attention.attention.value.weight          |   589824   |\n",
      "|          encoder.layer.10.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.10.attention.output.dense.weight           |   589824   |\n",
      "|            encoder.layer.10.attention.output.dense.bias            |    768     |\n",
      "|             encoder.layer.10.intermediate.dense.weight             |  2359296   |\n",
      "|              encoder.layer.10.intermediate.dense.bias              |    3072    |\n",
      "|                encoder.layer.10.output.dense.weight                |  2359296   |\n",
      "|                 encoder.layer.10.output.dense.bias                 |    768     |\n",
      "|              encoder.layer.10.layernorm_before.weight              |    768     |\n",
      "|               encoder.layer.10.layernorm_before.bias               |    768     |\n",
      "|              encoder.layer.10.layernorm_after.weight               |    768     |\n",
      "|               encoder.layer.10.layernorm_after.bias                |    768     |\n",
      "|         encoder.layer.11.attention.attention.query.weight          |   589824   |\n",
      "|          encoder.layer.11.attention.attention.query.bias           |    768     |\n",
      "|          encoder.layer.11.attention.attention.key.weight           |   589824   |\n",
      "|           encoder.layer.11.attention.attention.key.bias            |    768     |\n",
      "|         encoder.layer.11.attention.attention.value.weight          |   589824   |\n",
      "|          encoder.layer.11.attention.attention.value.bias           |    768     |\n",
      "|           encoder.layer.11.attention.output.dense.weight           |   589824   |\n",
      "|            encoder.layer.11.attention.output.dense.bias            |    768     |\n",
      "|             encoder.layer.11.intermediate.dense.weight             |  2359296   |\n",
      "|              encoder.layer.11.intermediate.dense.bias              |    3072    |\n",
      "|                encoder.layer.11.output.dense.weight                |  2359296   |\n",
      "|                 encoder.layer.11.output.dense.bias                 |    768     |\n",
      "|              encoder.layer.11.layernorm_before.weight              |    768     |\n",
      "|               encoder.layer.11.layernorm_before.bias               |    768     |\n",
      "|              encoder.layer.11.layernorm_after.weight               |    768     |\n",
      "|               encoder.layer.11.layernorm_after.bias                |    768     |\n",
      "|                          layernorm.weight                          |    768     |\n",
      "|                           layernorm.bias                           |    768     |\n",
      "|                        pooler.dense.weight                         |   589824   |\n",
      "|                         pooler.dense.bias                          |    768     |\n",
      "|                 embeddings.img_position_embedding                  |   967680   |\n",
      "|                  embeddings.embeddings.cls_token                   |    768     |\n",
      "|             embeddings.embeddings.position_embeddings              |   111360   |\n",
      "|    embeddings.embeddings.text_embeddings.word_embeddings.weight    |  23440896  |\n",
      "|  embeddings.embeddings.text_embeddings.position_embeddings.weight  |   30720    |\n",
      "| embeddings.embeddings.text_embeddings.token_type_embeddings.weight |    1536    |\n",
      "|       embeddings.embeddings.text_embeddings.LayerNorm.weight       |    768     |\n",
      "|        embeddings.embeddings.text_embeddings.LayerNorm.bias        |    768     |\n",
      "|      embeddings.embeddings.patch_embeddings.projection.weight      |  2359296   |\n",
      "|       embeddings.embeddings.patch_embeddings.projection.bias       |    768     |\n",
      "|         embeddings.embeddings.token_type_embeddings.weight         |    1536    |\n",
      "|                        classifier.0.weight                         |  1179648   |\n",
      "|                         classifier.0.bias                          |    1536    |\n",
      "|                        classifier.1.weight                         |    1536    |\n",
      "|                         classifier.1.bias                          |    1536    |\n",
      "|                        classifier.3.weight                         |   658944   |\n",
      "|                         classifier.3.bias                          |    429     |\n",
      "+--------------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 114406317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "114406317"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vilt.modeling_vilt import ViltEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.11'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "  (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "  (2): GELU(approximate='none')\n",
       "  (3): Linear(in_features=1536, out_features=429, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "isvqa = ISVQA(\"/home/nikostheodoridis/isvqa/train_set.json\",\n",
    "              \"/home/nikostheodoridis/nuscenes/samples\",\n",
    "              \"/home/nikostheodoridis/isvqa/answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(isvqa, 4, shuffle=False)\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 429])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = torch.randn(4, 1, 768)\n",
    "images = torch.randn(4, 6, 768)\n",
    "\n",
    "attn_scores = torch.randn(4, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikostheodoridis/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::copy_        46.67%      69.913ms        71.95%     107.799ms     128.638us     110.855ms        68.45%     110.855ms     132.285us           838  \n",
      "                                   aten::to         0.57%     857.000us        29.40%      44.043ms     205.808us       1.066ms         0.66%      44.273ms     206.883us           214  \n",
      "                             aten::_to_copy         1.20%       1.795ms        28.79%      43.127ms     201.528us       1.437ms         0.89%      43.207ms     201.902us           214  \n",
      "                               aten::detach        15.07%      22.581ms        17.18%      25.740ms       4.414us      25.315ms        15.63%      36.165ms       6.202us          5831  \n",
      "                                     detach         0.59%     889.000us         0.59%     889.000us       0.152us      10.857ms         6.70%      10.857ms       1.862us          5831  \n",
      "                                aten::empty         1.49%       2.236ms         1.49%       2.236ms       1.762us       5.157ms         3.18%       5.157ms       4.064us          1269  \n",
      "                        aten::empty_strided         0.42%     635.000us         1.67%       2.504ms      11.701us       3.675ms         2.27%       3.675ms      17.173us           214  \n",
      "                                aten::fill_         0.83%       1.243ms         0.83%       1.243ms      15.159us       1.340ms         0.83%       1.340ms      16.341us            82  \n",
      "                                 aten::set_         0.06%      90.000us         0.06%      90.000us       0.141us       1.244ms         0.77%       1.244ms       1.947us           639  \n",
      "                                aten::zero_         0.03%      46.000us         0.59%     884.000us      10.045us     289.000us         0.18%     902.000us      10.250us            88  \n",
      "                                aten::zeros         0.05%      76.000us         0.64%     956.000us     106.222us      57.000us         0.04%     711.000us      79.000us             9  \n",
      "    aten::_has_compatible_shallow_copy_type         0.00%       2.000us         0.00%       2.000us       0.005us     453.000us         0.28%     453.000us       1.068us           424  \n",
      "                               aten::arange         0.06%      83.000us         0.09%     140.000us      23.333us     179.000us         0.11%     218.000us      36.333us             6  \n",
      "                               aten::expand         0.03%      41.000us         0.03%      47.000us      15.667us      20.000us         0.01%      27.000us       9.000us             3  \n",
      "                            cudaEventRecord         6.30%       9.441ms         6.30%       9.441ms       0.301us       0.000us         0.00%      12.000us       0.000us         31336  \n",
      "                              aten::resize_         0.00%       3.000us         0.00%       3.000us       1.000us       7.000us         0.00%       7.000us       2.333us             3  \n",
      "                           aten::as_strided         0.00%       6.000us         0.00%       6.000us       2.000us       7.000us         0.00%       7.000us       2.333us             3  \n",
      "           cudaDeviceGetStreamPriorityRange         0.00%       1.000us         0.00%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                         cudaGetDeviceCount         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                      cudaStreamIsCapturing         0.00%       5.000us         0.00%       5.000us       0.200us       0.000us         0.00%       0.000us       0.000us            25  \n",
      "                 cudaGetDeviceProperties_v2         0.03%      51.000us         0.03%      51.000us      51.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                 cudaMalloc         1.25%       1.867ms         1.25%       1.867ms      84.864us       0.000us         0.00%       0.000us       0.000us            22  \n",
      "                            cudaMemcpyAsync        21.63%      32.411ms        21.63%      32.411ms     151.453us       0.000us         0.00%       0.000us       0.000us           214  \n",
      "                      cudaStreamSynchronize         3.70%       5.541ms         3.70%       5.541ms      25.893us       0.000us         0.00%       0.000us       0.000us           214  \n",
      "                      cudaDeviceSynchronize         0.00%       3.000us         0.00%       3.000us       3.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 149.816ms\n",
      "Self CUDA time total: 161.958ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=True) as prof:\n",
    "    model = MultiviewViltForQuestionAnsweringBaseline(6, 210, 768, True, True, False).to(\"cuda\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(isvqa, 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.Size' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1913, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = torch.randn(1, 768)\n",
    "images = torch.randn(6, 768)\n",
    "\n",
    "img_attn = nn.MultiheadAttention(768, 12)\n",
    "\n",
    "_, attn_scores = img_attn(questions, images, images)\n",
    "\n",
    "attn_scores[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model, loader, acc_fn, answ_len):\n",
    "    \"\"\"\n",
    "    A function that validates the model by going through all the mini-batches in the validation dataloader once.\n",
    "    \"\"\"\n",
    "    print(\"\\tValidating...\")\n",
    "    model.eval()\n",
    "    losses = []  # to save the loss of each mini-batch in order to take their average at the end\n",
    "    accuracies = []  # to save the accuracy of each mini-batch in order to take their average at the end\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (X, y) in enumerate(loader):\n",
    "            outputs = model(**X, labels=y)\n",
    "            loss = outputs.loss\n",
    "            pred = max_to_one_hot(outputs.logits)\n",
    "            acc = acc_fn(pred, y, answ_len)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(acc)\n",
    "\n",
    "    avg_loss = sum(losses) / len(loader)\n",
    "    avg_acc = sum(accuracies) / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikostheodoridis/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MultiviewViltForQuestionAnswering(6, 210, 768, True, False, False).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.classifier = nn.Sequential(\n",
    "        nn.Linear(768, 1536),\n",
    "        nn.LayerNorm(1536),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(1536, 429)\n",
    "    ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.load_state_dict(torch.load(\"/home/nikostheodoridis/Trained Models/2024-07-08 00:07:49/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p1, p2 in zip(model.parameters(), trained_model.parameters()):\n",
    "#     assert torch.equal(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = ISVQA(qa_path=\"/home/nikostheodoridis/isvqa/val_set.json\",\n",
    "                nuscenes_path=\"/home/nikostheodoridis/nuscenes/samples\",\n",
    "                answers_path=\"/home/nikostheodoridis/isvqa/answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_set, batch_size=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = []\n",
    "# untrained_predictions = []\n",
    "# trained_predictions = []\n",
    "\n",
    "# model.eval()\n",
    "# trained_model.eval()\n",
    "# for i in range(2576):\n",
    "#     inputs, target = val_set[i]\n",
    "\n",
    "#     targets.append(target)\n",
    "\n",
    "#     with torch.inference_mode():\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions: torch.Tensor, targets: torch.Tensor, answers_len: int) -> float:\n",
    "    cnt = torch.eq(torch.eq(predictions, targets).sum(dim=1), answers_len).sum()\n",
    "    return cnt.item() / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidating...\n"
     ]
    }
   ],
   "source": [
    "untrained_loss, untrained_acc = val_step(model, val_loader, accuracy, 429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidating...\n"
     ]
    }
   ],
   "source": [
    "trained_loss, trained_acc = val_step(trained_model, val_loader, accuracy, 429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303.8099614342978\n",
      "0.0011627906976744186\n"
     ]
    }
   ],
   "source": [
    "print(untrained_loss)\n",
    "print(untrained_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5099486532945967\n",
      "0.6003875968992246\n"
     ]
    }
   ],
   "source": [
    "print(trained_loss)\n",
    "print(trained_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nikostheodoridis/isvqa/answers_counter.json\") as f:\n",
    "    answers_cnt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'yes': 13564,\n",
       "         'no': 3734,\n",
       "         'one': 3663,\n",
       "         'white': 2893,\n",
       "         'two': 2544,\n",
       "         'red': 1205,\n",
       "         'black': 1046,\n",
       "         'blue': 1025,\n",
       "         'three': 986,\n",
       "         'green': 968,\n",
       "         'yellow': 930,\n",
       "         'orange': 782,\n",
       "         'four': 529,\n",
       "         'night': 464,\n",
       "         'rainy': 434,\n",
       "         'gray': 365,\n",
       "         'black and white': 345,\n",
       "         'silver': 287,\n",
       "         'zero': 254,\n",
       "         'five': 218,\n",
       "         'orange and white': 178,\n",
       "         'six': 156,\n",
       "         'left': 151,\n",
       "         'none': 147,\n",
       "         'ahead': 147,\n",
       "         'right': 141,\n",
       "         'fedex': 136,\n",
       "         'brown': 134,\n",
       "         'cloudy': 129,\n",
       "         'slow': 119,\n",
       "         'ups': 114,\n",
       "         'bus': 112,\n",
       "         'raining': 111,\n",
       "         'wet': 111,\n",
       "         'sunny': 107,\n",
       "         'ryder': 95,\n",
       "         'urban': 93,\n",
       "         'twenty-three': 92,\n",
       "         'stop': 89,\n",
       "         'day': 77,\n",
       "         'hump': 69,\n",
       "         'brick': 69,\n",
       "         'red and white': 57,\n",
       "         'twenty-five': 55,\n",
       "         'bridge': 55,\n",
       "         'rectangle': 54,\n",
       "         'trees': 53,\n",
       "         'truck': 52,\n",
       "         'city': 51,\n",
       "         'toyota': 50,\n",
       "         'nineteen': 48,\n",
       "         'on': 46,\n",
       "         'rain': 40,\n",
       "         'seven': 39,\n",
       "         'forty': 37,\n",
       "         'french': 36,\n",
       "         'black and yellow': 36,\n",
       "         'innosparks': 35,\n",
       "         'square': 34,\n",
       "         'ford': 34,\n",
       "         'blue and green': 34,\n",
       "         'umbrella': 33,\n",
       "         'night time': 33,\n",
       "         'metal': 32,\n",
       "         'arrow': 32,\n",
       "         'van': 31,\n",
       "         'motorcycle': 29,\n",
       "         'parked': 29,\n",
       "         'fence': 29,\n",
       "         'construction': 28,\n",
       "         'nighttime': 28,\n",
       "         'p': 28,\n",
       "         'eversource energy': 28,\n",
       "         'green and blue': 28,\n",
       "         'nuss': 27,\n",
       "         'car': 27,\n",
       "         'crosswalk': 27,\n",
       "         'boston': 26,\n",
       "         'dark': 26,\n",
       "         'one way': 26,\n",
       "         'cars': 25,\n",
       "         'tree': 25,\n",
       "         'yankee': 25,\n",
       "         'do not enter': 25,\n",
       "         'water': 25,\n",
       "         'building': 25,\n",
       "         'twelve': 24,\n",
       "         'twenty-one': 24,\n",
       "         'pink': 24,\n",
       "         'suv': 23,\n",
       "         'starbucks': 23,\n",
       "         'north coast': 23,\n",
       "         'bicycle': 23,\n",
       "         'blue and white': 23,\n",
       "         'john nagle': 23,\n",
       "         'hump ahead': 22,\n",
       "         'eight': 22,\n",
       "         'behind': 22,\n",
       "         'bike': 21,\n",
       "         'xfinity': 21,\n",
       "         'sidewalk': 21,\n",
       "         'm': 20,\n",
       "         'harbor place': 20,\n",
       "         'usps': 20,\n",
       "         'both': 20,\n",
       "         'round': 20,\n",
       "         'parking lot': 20,\n",
       "         'white and orange': 19,\n",
       "         'orange and gray': 19,\n",
       "         'only': 19,\n",
       "         'stop sign': 19,\n",
       "         'ninety': 19,\n",
       "         'chain link': 19,\n",
       "         'penske': 19,\n",
       "         'fusionopolis two': 19,\n",
       "         'jeep': 18,\n",
       "         'holland ave': 18,\n",
       "         'gemalto': 18,\n",
       "         'white and yellow': 18,\n",
       "         'concrete': 18,\n",
       "         'hood': 18,\n",
       "         'circle': 17,\n",
       "         'gray and orange': 17,\n",
       "         'solid': 17,\n",
       "         'river': 17,\n",
       "         'synthesis': 17,\n",
       "         'backpack': 17,\n",
       "         'nine': 16,\n",
       "         'row thirty-four': 16,\n",
       "         'grass': 16,\n",
       "         'sportello': 16,\n",
       "         'yellow and black': 16,\n",
       "         'starbucks coffee': 16,\n",
       "         'dhl': 16,\n",
       "         'residential': 16,\n",
       "         'v': 16,\n",
       "         'caution': 15,\n",
       "         'one north gateway': 15,\n",
       "         'back right': 15,\n",
       "         'dog': 15,\n",
       "         'clouds': 15,\n",
       "         'crane': 15,\n",
       "         'nucleos': 15,\n",
       "         'bus stop': 14,\n",
       "         'closed': 14,\n",
       "         'nine hundred and seventy': 14,\n",
       "         'fire hydrant': 14,\n",
       "         'one north link': 14,\n",
       "         'open': 14,\n",
       "         'harvey': 13,\n",
       "         'glass': 13,\n",
       "         'bricks': 13,\n",
       "         'worldwide services': 13,\n",
       "         'speedpost': 12,\n",
       "         'walking': 12,\n",
       "         'road': 12,\n",
       "         'wheelchair': 12,\n",
       "         'museum': 12,\n",
       "         'seventy-five': 12,\n",
       "         'more': 12,\n",
       "         'innovationsquareboston': 12,\n",
       "         'yankee lobster': 12,\n",
       "         'sedan': 11,\n",
       "         'rectangular': 11,\n",
       "         'yellow and white': 11,\n",
       "         'x ing': 11,\n",
       "         'harpoon brewery': 11,\n",
       "         'evening': 11,\n",
       "         'canteen': 11,\n",
       "         'business link': 11,\n",
       "         'down': 11,\n",
       "         'ladder': 10,\n",
       "         'front': 10,\n",
       "         'ten': 10,\n",
       "         'wood': 10,\n",
       "         'msc': 10,\n",
       "         'overcast': 10,\n",
       "         'barbed wire': 10,\n",
       "         'two way': 10,\n",
       "         'shorts': 10,\n",
       "         'woman': 10,\n",
       "         'cat': 9,\n",
       "         'three way': 9,\n",
       "         'sysco': 9,\n",
       "         'centros': 9,\n",
       "         'light': 9,\n",
       "         'parking garage': 9,\n",
       "         'x': 9,\n",
       "         'empty': 9,\n",
       "         'works ahead': 9,\n",
       "         'ninety-three': 9,\n",
       "         'mercedes': 9,\n",
       "         'diamond': 9,\n",
       "         'biopolis dr': 9,\n",
       "         'front left': 9,\n",
       "         'cones': 9,\n",
       "         'commercial': 9,\n",
       "         'dunkin donuts': 9,\n",
       "         'seafood': 9,\n",
       "         'suffolk': 9,\n",
       "         's': 9,\n",
       "         'skanska': 9,\n",
       "         '': 8,\n",
       "         'mass bay credit union': 8,\n",
       "         'one hundred and fifty-one': 8,\n",
       "         'triangle': 8,\n",
       "         'four hundred and fifty-one': 8,\n",
       "         'five eleven': 8,\n",
       "         'macco energy': 8,\n",
       "         'mail truck': 8,\n",
       "         'back': 8,\n",
       "         'honda': 8,\n",
       "         'helmet': 8,\n",
       "         'nus': 8,\n",
       "         'double': 8,\n",
       "         'industrial': 8,\n",
       "         'squares': 8,\n",
       "         'bag': 8,\n",
       "         'taxi': 8,\n",
       "         'stopped': 7,\n",
       "         'four way': 7,\n",
       "         'whiskey priest': 7,\n",
       "         'traffic lights': 7,\n",
       "         'go': 7,\n",
       "         'morning': 7,\n",
       "         'usa': 7,\n",
       "         'reduce speed now': 7,\n",
       "         'dry': 7,\n",
       "         'green and white': 7,\n",
       "         'gold': 7,\n",
       "         'bicycles': 7,\n",
       "         'we deliver for you': 7,\n",
       "         'eleven': 7,\n",
       "         'bench': 7,\n",
       "         'innovation and design': 7,\n",
       "         'legal': 7,\n",
       "         'la casa de pedro': 7,\n",
       "         'sandcrawler': 7,\n",
       "         'traffic cones': 7,\n",
       "         'fort point market': 7,\n",
       "         'exit only': 7,\n",
       "         'red and gray': 7,\n",
       "         'vpne': 7,\n",
       "         'autodesk': 6,\n",
       "         'scooter': 6,\n",
       "         'rounded': 6,\n",
       "         'brown and white': 6,\n",
       "         'stars ave': 6,\n",
       "         'female': 6,\n",
       "         'enterprise': 6,\n",
       "         'twenty-two': 6,\n",
       "         'orange and black': 6,\n",
       "         'forward': 6,\n",
       "         'white and black': 6,\n",
       "         'male': 6,\n",
       "         \"don't walk\": 6,\n",
       "         'bmw': 6,\n",
       "         'red light': 6,\n",
       "         'blue harvest': 6,\n",
       "         'plants': 6,\n",
       "         'to right': 6,\n",
       "         'dashed': 6,\n",
       "         'red and blue': 6,\n",
       "         'available liftgate service': 6,\n",
       "         'www': 6,\n",
       "         'away': 6,\n",
       "         'boston freight terminals': 6,\n",
       "         'parking': 6,\n",
       "         'flett': 6,\n",
       "         'rosa mexicano': 5,\n",
       "         'on sidewalk': 5,\n",
       "         'back left': 5,\n",
       "         'cone': 5,\n",
       "         'isuzu': 5,\n",
       "         'windows': 5,\n",
       "         'sbwtc': 5,\n",
       "         'lights': 5,\n",
       "         'american': 5,\n",
       "         'portsdown rd': 5,\n",
       "         'e': 5,\n",
       "         'man': 5,\n",
       "         'true': 5,\n",
       "         'dark blue': 5,\n",
       "         'flowers': 5,\n",
       "         'arch': 5,\n",
       "         'wagamama': 5,\n",
       "         'fifteen': 5,\n",
       "         'curved': 5,\n",
       "         'kent ridge drive': 5,\n",
       "         'd': 5,\n",
       "         '4x4': 5,\n",
       "         'clear': 5,\n",
       "         'railroad crossing': 5,\n",
       "         'moving': 5,\n",
       "         'ayer rajah': 5,\n",
       "         'sitting': 5,\n",
       "         'phone': 5,\n",
       "         'salmon': 5,\n",
       "         'parking meter': 5,\n",
       "         'person': 5,\n",
       "         'straight': 5,\n",
       "         'xpo': 5,\n",
       "         'supercamp': 5,\n",
       "         'minivan': 5,\n",
       "         'tcoms': 5,\n",
       "         'factory outlet': 5,\n",
       "         'english': 5,\n",
       "         'palm tree': 5,\n",
       "         'mediapolis': 5,\n",
       "         'sign': 5,\n",
       "         'audi': 5,\n",
       "         'thirty-six': 5,\n",
       "         'seaport world trade center': 5,\n",
       "         'mail': 5,\n",
       "         'all': 5,\n",
       "         'construction ahead': 4,\n",
       "         'suburban': 4,\n",
       "         'mitsubishi': 4,\n",
       "         'bush': 4,\n",
       "         'sunglasses': 4,\n",
       "         'yang ming': 4,\n",
       "         'blue and yellow': 4,\n",
       "         'mitsubishi electric': 4,\n",
       "         'double decker bus': 4,\n",
       "         'towards': 4,\n",
       "         'up': 4,\n",
       "         'legal harborside': 4,\n",
       "         'lobster': 4,\n",
       "         'fios': 4,\n",
       "         'radius bank': 4,\n",
       "         'standing': 4,\n",
       "         'arrows': 4,\n",
       "         'triangles': 4,\n",
       "         'lexus': 4,\n",
       "         'green and red': 4,\n",
       "         'false': 4,\n",
       "         'nothing': 4,\n",
       "         'pedestrians': 4,\n",
       "         'gateway': 4,\n",
       "         'cranes': 4,\n",
       "         'paul w': 4,\n",
       "         'harpoon': 4,\n",
       "         'smoke shop': 4,\n",
       "         'red and green': 4,\n",
       "         'donuts': 4,\n",
       "         'green and orange': 4,\n",
       "         'jeans': 4,\n",
       "         'blue and orange': 4,\n",
       "         'singapore science park': 4,\n",
       "         'ship': 4,\n",
       "         'alumni house': 4,\n",
       "         'on road': 4,\n",
       "         'stroller': 4,\n",
       "         'exit': 4,\n",
       "         'bushes': 4,\n",
       "         'detour': 4,\n",
       "         'blue harvest fisheries': 4,\n",
       "         'partly cloudy': 4,\n",
       "         'w': 4,\n",
       "         'bank of america': 4,\n",
       "         'white car': 4,\n",
       "         'cell phone': 4,\n",
       "         'innovation': 4,\n",
       "         'mountain': 4,\n",
       "         'excavator': 3,\n",
       "         'yield': 3,\n",
       "         'three hundred': 3,\n",
       "         'thirty-five': 3,\n",
       "         'mazda': 3,\n",
       "         'dusk': 3,\n",
       "         'sun': 3,\n",
       "         'beige': 3,\n",
       "         'crossing street': 3,\n",
       "         'singapore': 3,\n",
       "         'outside': 3,\n",
       "         'bad': 3,\n",
       "         'site access': 3,\n",
       "         'dead end': 3,\n",
       "         'geico': 3,\n",
       "         'four hundred and eleven': 3,\n",
       "         'stairs': 3,\n",
       "         'low': 3,\n",
       "         'h': 3,\n",
       "         'sahara': 3,\n",
       "         'chevy': 3,\n",
       "         'buildings': 3,\n",
       "         'sbs transit': 3,\n",
       "         'six hundred and seventeen five hundred and twenty-three eight thousand': 3,\n",
       "         'fusionopolis': 3,\n",
       "         'traffic light': 3,\n",
       "         'daylight': 3,\n",
       "         'self storage': 3,\n",
       "         'palm trees': 3,\n",
       "         'american flag': 3,\n",
       "         'short': 3,\n",
       "         'seventy-nine': 3,\n",
       "         'biopolis rd': 3,\n",
       "         'street': 3,\n",
       "         'mandm': 3,\n",
       "         'blue dragon': 3,\n",
       "         'humps': 3,\n",
       "         'twenty': 3,\n",
       "         'white and red': 3,\n",
       "         't': 3,\n",
       "         'mailbox': 3,\n",
       "         'peapod': 3,\n",
       "         'congress st': 3,\n",
       "         'future starts here': 3,\n",
       "         'park': 3,\n",
       "         'large': 3,\n",
       "         'construction workers': 3,\n",
       "         'mirror': 3,\n",
       "         'hat': 3,\n",
       "         'twenty-eight': 3,\n",
       "         'foley': 3,\n",
       "         '02148m0003': 3,\n",
       "         'tan': 3,\n",
       "         'synapse': 3,\n",
       "         'business': 3,\n",
       "         'green and yellow': 3,\n",
       "         'pickup truck': 3,\n",
       "         'pandg': 3,\n",
       "         'dark gray': 3,\n",
       "         'intersection': 3,\n",
       "         'public parking': 3,\n",
       "         'for lease': 3,\n",
       "         'striped': 3,\n",
       "         'light blue': 3,\n",
       "         'trucks': 3})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(answers_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Start\n",
    "import json\n",
    "import random\n",
    "train_path = \"/home/nikostheodoridis/nuscenes-qa/train_set.json\"\n",
    "\n",
    "val_path = \"/home/nikostheodoridis/nuscenes-qa/val_set.json\"\n",
    "\n",
    "test_path = \"/home/nikostheodoridis/nuscenes-qa/test_set.json\"\n",
    "with open(train_path) as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(val_path) as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open(test_path) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "for data in train_data:\n",
    "    if data in test_data:\n",
    "        print(\"False\")\n",
    "        break\n",
    "else:\n",
    "    print(\"True\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
