{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from utility import ViltImageSetProcessor\n",
    "from transformers import ViltImageProcessor, BertTokenizer\n",
    "from models import MultiviewViltForQuestionAnswering, MultiviewViltModel\n",
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "from isvqa_data_setup import ISVQA\n",
    "from torch.utils.data import DataLoader\n",
    "from engine import max_to_one_hot\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "with open(\"/home/nikostheodoridis/nuscenes/v1.0-trainval/sample_data.json\") as f:\n",
    "    set = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': 'f14823d484164684b9f25c34f72016a0',\n",
       "  'sample_token': '688f86c9082242edaa9f4b19d759e3e0',\n",
       "  'ego_pose_token': 'f14823d484164684b9f25c34f72016a0',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886323297423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886323297423.jpg',\n",
       "  'prev': 'bbd1e3c82b87480d8e37e3c5bc625bc8',\n",
       "  'next': 'f23af351308f4ba8a292ff6a07a5c734'},\n",
       " {'token': 'ad7e2bedd23f44f0bc9997c60c6605f8',\n",
       "  'sample_token': '2e62af9882e144f8adc3d60bbd68804b',\n",
       "  'ego_pose_token': 'ad7e2bedd23f44f0bc9997c60c6605f8',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886322947423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886322947423.jpg',\n",
       "  'prev': '5e3a87daf006462a9db497734761b021',\n",
       "  'next': '020b2fa4246149b7b72dd1eae5ac0b4b'},\n",
       " {'token': '8f4a2a4bca85494788792474dbcb4947',\n",
       "  'sample_token': 'cb8fbf228afd41fcb6e81ae7fe18ba61',\n",
       "  'ego_pose_token': '8f4a2a4bca85494788792474dbcb4947',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886324797423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886324797423.jpg',\n",
       "  'prev': '45ae8865a8bd4c3b8c9945ec5128fc35',\n",
       "  'next': 'bd737819f25e40d9858bb76f2db17175'},\n",
       " {'token': '12b9ed7da83a43e49b47c4d2158e155f',\n",
       "  'sample_token': '7f768fc8681d448db27638d18914d991',\n",
       "  'ego_pose_token': '12b9ed7da83a43e49b47c4d2158e155f',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886341437525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886341437525.jpg',\n",
       "  'prev': '541ed403c9804acc8c54790a044dea63',\n",
       "  'next': '43f29355b11c49e4a863422c43b33040'},\n",
       " {'token': '4df4889cf5e447c1ac9c6016d9deefc3',\n",
       "  'sample_token': '13b659ff8b004f8b9285cefeb975af49',\n",
       "  'ego_pose_token': '4df4889cf5e447c1ac9c6016d9deefc3',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886339637525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886339637525.jpg',\n",
       "  'prev': '2d6333e7eb4046b9b751f3cbfdf79572',\n",
       "  'next': 'ff3232b8a1cf470aae400ae11fe48447'},\n",
       " {'token': 'edffe7435b534c11be2f966ff110f99f',\n",
       "  'sample_token': '92fd53d745444023a73c0dd24c1edec7',\n",
       "  'ego_pose_token': 'edffe7435b534c11be2f966ff110f99f',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886340437525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886340437525.jpg',\n",
       "  'prev': '47b1af5c37c24d6fb6d7aa950da6d150',\n",
       "  'next': '3cc00ad6b241428483d6176656b46e7f'},\n",
       " {'token': 'd1e7f84efe38478d9294292f0ad6989d',\n",
       "  'sample_token': '13b659ff8b004f8b9285cefeb975af49',\n",
       "  'ego_pose_token': 'd1e7f84efe38478d9294292f0ad6989d',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886339887525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': True,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'samples/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886339887525.jpg',\n",
       "  'prev': 'de083f5041334c99a10437189b45bba1',\n",
       "  'next': '7b436ee68cf14290b0c9efcac2e6e545'},\n",
       " {'token': 'c88e17ffc29d432690278d48aa66bbb4',\n",
       "  'sample_token': 'f48a2f3579984916b433c86bd1ed849d',\n",
       "  'ego_pose_token': 'c88e17ffc29d432690278d48aa66bbb4',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886327897423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886327897423.jpg',\n",
       "  'prev': '58b23133e6db4cd8ae07174d7faaf937',\n",
       "  'next': 'c8a2f6255c9543cc80c46bdddaa3edc4'},\n",
       " {'token': '57ba6c34505d4b5d9f1d3ec6b37312e2',\n",
       "  'sample_token': '96ee33f7aa5e48bbab4b36f7ceead005',\n",
       "  'ego_pose_token': '57ba6c34505d4b5d9f1d3ec6b37312e2',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886323948527,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886323948527.jpg',\n",
       "  'prev': '20744e40252e4d5c849a2fbea22b4a1d',\n",
       "  'next': '65ea0ec3bccb4d58bbe1a3821f27129d'},\n",
       " {'token': '5b5f8a4a47204677bd0b79a1a603b765',\n",
       "  'sample_token': 'ff701e8d5743462ba9f6bc60eff70ab9',\n",
       "  'ego_pose_token': '5b5f8a4a47204677bd0b79a1a603b765',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886325447423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': True,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'samples/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886325447423.jpg',\n",
       "  'prev': 'bd0dd457a4d247519c33c884fcfc84f9',\n",
       "  'next': 'c6e8f0f8425f495ba943d8624822cd7c'},\n",
       " {'token': '7e8f6791e2534f9d97bf870c0ca887c8',\n",
       "  'sample_token': '2e62af9882e144f8adc3d60bbd68804b',\n",
       "  'ego_pose_token': '7e8f6791e2534f9d97bf870c0ca887c8',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886322697423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886322697423.jpg',\n",
       "  'prev': '08aaa187286e47218219c1a018323dc8',\n",
       "  'next': '4ec101e1b38d4648a5257996ec775619'},\n",
       " {'token': '8577c0d7061841f9b8b0414606937eee',\n",
       "  'sample_token': '96ee33f7aa5e48bbab4b36f7ceead005',\n",
       "  'ego_pose_token': '8577c0d7061841f9b8b0414606937eee',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886323647423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886323647423.jpg',\n",
       "  'prev': 'e56aa5dab4bb4d3e96071e75ef0c9312',\n",
       "  'next': 'eb1f8990fa4d4b59a4431dca9858d980'},\n",
       " {'token': 'a7675cdb09974de89ad6bb3f2e030e01',\n",
       "  'sample_token': 'fa705f3462f4473b82ad8c86f1bdf90a',\n",
       "  'ego_pose_token': 'a7675cdb09974de89ad6bb3f2e030e01',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886340287525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886340287525.jpg',\n",
       "  'prev': '8914da415000492183e50083bd602cb9',\n",
       "  'next': '47b1af5c37c24d6fb6d7aa950da6d150'},\n",
       " {'token': 'eeb3bcbd510049b9902b52eaaf49e08c',\n",
       "  'sample_token': '0b3a5293539a4cd5933ef42edc7200d2',\n",
       "  'ego_pose_token': 'eeb3bcbd510049b9902b52eaaf49e08c',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886341137525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886341137525.jpg',\n",
       "  'prev': '49b343f97db34dc09c04ea2a4c7695c9',\n",
       "  'next': '3adba2f977244cc58b5461870acc3582'},\n",
       " {'token': '3cc00ad6b241428483d6176656b46e7f',\n",
       "  'sample_token': '92fd53d745444023a73c0dd24c1edec7',\n",
       "  'ego_pose_token': '3cc00ad6b241428483d6176656b46e7f',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886340537525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886340537525.jpg',\n",
       "  'prev': 'edffe7435b534c11be2f966ff110f99f',\n",
       "  'next': '5a64302a69cc466099cdcc51e001c18c'},\n",
       " {'token': '4984f5c2cdc84bb3841e74bb4b360049',\n",
       "  'sample_token': '43d5d31802ee447e9c25966c45e7b698',\n",
       "  'ego_pose_token': '4984f5c2cdc84bb3841e74bb4b360049',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886328547423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886328547423.jpg',\n",
       "  'prev': '281987caeb39487eb08162341eebfb9a',\n",
       "  'next': 'e482f57cbe40453897e0b1b761490ab3'},\n",
       " {'token': '9c88282772d94928994fa0da58a91735',\n",
       "  'sample_token': '0cd37221c4724bfc8e1c369278c49d03',\n",
       "  'ego_pose_token': '9c88282772d94928994fa0da58a91735',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886326147423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886326147423.jpg',\n",
       "  'prev': 'd0c2743bb6284ff6825f21e5a76ceae2',\n",
       "  'next': '34aee8afb41d40a0b96b48f114010cc2'},\n",
       " {'token': '84f78a9585a24c119201126dd4685fcd',\n",
       "  'sample_token': 'cb8fbf228afd41fcb6e81ae7fe18ba61',\n",
       "  'ego_pose_token': '84f78a9585a24c119201126dd4685fcd',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886324647423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886324647423.jpg',\n",
       "  'prev': 'b55f8207e77e4c21b31b348713116370',\n",
       "  'next': '45ae8865a8bd4c3b8c9945ec5128fc35'},\n",
       " {'token': 'f23af351308f4ba8a292ff6a07a5c734',\n",
       "  'sample_token': '688f86c9082242edaa9f4b19d759e3e0',\n",
       "  'ego_pose_token': 'f23af351308f4ba8a292ff6a07a5c734',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886323397423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886323397423.jpg',\n",
       "  'prev': 'f14823d484164684b9f25c34f72016a0',\n",
       "  'next': '1ea12ac271314e6fa71461050a6a9853'},\n",
       " {'token': 'eb65894d06ba40eb83d28a77445bf259',\n",
       "  'sample_token': 'ba20a7743bae4d2dabc6be640c66ccc7',\n",
       "  'ego_pose_token': 'eb65894d06ba40eb83d28a77445bf259',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886324297423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886324297423.jpg',\n",
       "  'prev': 'b7c5baba5d9b4fe5a7cadedd737c9792',\n",
       "  'next': 'f9892f70be94477295b1d634ae340897'},\n",
       " {'token': '5ed19e73fcde4f15b8156d2d935fb501',\n",
       "  'sample_token': '0b3a5293539a4cd5933ef42edc7200d2',\n",
       "  'ego_pose_token': '5ed19e73fcde4f15b8156d2d935fb501',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886340937525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886340937525.jpg',\n",
       "  'prev': '6f020b742d214d6985cd0509206199c4',\n",
       "  'next': '49b343f97db34dc09c04ea2a4c7695c9'},\n",
       " {'token': '3adba2f977244cc58b5461870acc3582',\n",
       "  'sample_token': '0b3a5293539a4cd5933ef42edc7200d2',\n",
       "  'ego_pose_token': '3adba2f977244cc58b5461870acc3582',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886341187525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886341187525.jpg',\n",
       "  'prev': 'eeb3bcbd510049b9902b52eaaf49e08c',\n",
       "  'next': 'f85de7863f9549e496b4acf2f7fe5ea2'},\n",
       " {'token': '2f90a5fd68a84f4e9e51afc54c6d95e2',\n",
       "  'sample_token': '7f768fc8681d448db27638d18914d991',\n",
       "  'ego_pose_token': '2f90a5fd68a84f4e9e51afc54c6d95e2',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886341787525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886341787525.jpg',\n",
       "  'prev': 'fa7dc186b1f549758c2bd4a71a1c3aaa',\n",
       "  'next': '124d05beb4b64145a90c6d273dd29fe9'},\n",
       " {'token': '8b7249492c1846dfb958e9d49c084748',\n",
       "  'sample_token': '9ca3ae3c6a8b4776b8543bb94f76ce5f',\n",
       "  'ego_pose_token': '8b7249492c1846dfb958e9d49c084748',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886329197423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886329197423.jpg',\n",
       "  'prev': '0a7c8136df344d168397a385e75d7a90',\n",
       "  'next': '9b6146a0eec14338a45ade49b9ce8cae'},\n",
       " {'token': 'aa6a44a39b4a4be1aafe127d7bd7e81f',\n",
       "  'sample_token': '868c93c6935944fab6366fcc5de80dfb',\n",
       "  'ego_pose_token': 'aa6a44a39b4a4be1aafe127d7bd7e81f',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886326797423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886326797423.jpg',\n",
       "  'prev': 'e3b748a1343f4bcfb708bff882478b1d',\n",
       "  'next': '72fc98bbce04404e8cc18d5adc729b24'},\n",
       " {'token': 'ade41386daea4f738cc49bf94edb2e9e',\n",
       "  'sample_token': 'ff701e8d5743462ba9f6bc60eff70ab9',\n",
       "  'ego_pose_token': 'ade41386daea4f738cc49bf94edb2e9e',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886325297423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886325297423.jpg',\n",
       "  'prev': 'ab41d4edc6144829972e1e68effe6e8c',\n",
       "  'next': 'bd0dd457a4d247519c33c884fcfc84f9'},\n",
       " {'token': '65ea0ec3bccb4d58bbe1a3821f27129d',\n",
       "  'sample_token': '96ee33f7aa5e48bbab4b36f7ceead005',\n",
       "  'ego_pose_token': '65ea0ec3bccb4d58bbe1a3821f27129d',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886324047423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': True,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'samples/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886324047423.jpg',\n",
       "  'prev': '57ba6c34505d4b5d9f1d3ec6b37312e2',\n",
       "  'next': 'a1f3c685b4414fbc92e714bb430c8b27'},\n",
       " {'token': '4dfe0d875a2e421ab3c57280398eea68',\n",
       "  'sample_token': 'cb8fbf228afd41fcb6e81ae7fe18ba61',\n",
       "  'ego_pose_token': '4dfe0d875a2e421ab3c57280398eea68',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886324947423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886324947423.jpg',\n",
       "  'prev': 'bd737819f25e40d9858bb76f2db17175',\n",
       "  'next': '93d4fe1d85004141806db5aad53b7460'},\n",
       " {'token': 'be144f245a034e8e80b5103ce957441b',\n",
       "  'sample_token': '7f768fc8681d448db27638d18914d991',\n",
       "  'ego_pose_token': 'be144f245a034e8e80b5103ce957441b',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886341637525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886341637525.jpg',\n",
       "  'prev': '43f29355b11c49e4a863422c43b33040',\n",
       "  'next': 'fa7dc186b1f549758c2bd4a71a1c3aaa'},\n",
       " {'token': '124d05beb4b64145a90c6d273dd29fe9',\n",
       "  'sample_token': '7f768fc8681d448db27638d18914d991',\n",
       "  'ego_pose_token': '124d05beb4b64145a90c6d273dd29fe9',\n",
       "  'calibrated_sensor_token': '38f7c900c147455f9ead5ca01f655858',\n",
       "  'timestamp': 1531886341887525,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': True,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'samples/CAM_BACK/n015-2018-07-18-11-50-34+0800__CAM_BACK__1531886341887525.jpg',\n",
       "  'prev': '2f90a5fd68a84f4e9e51afc54c6d95e2',\n",
       "  'next': ''},\n",
       " {'token': '020b2fa4246149b7b72dd1eae5ac0b4b',\n",
       "  'sample_token': '2e62af9882e144f8adc3d60bbd68804b',\n",
       "  'ego_pose_token': '020b2fa4246149b7b72dd1eae5ac0b4b',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886323047423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': True,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'samples/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886323047423.jpg',\n",
       "  'prev': 'ad7e2bedd23f44f0bc9997c60c6605f8',\n",
       "  'next': 'c93d1c4074aa4757a3e80cc882cc4ee7'},\n",
       " {'token': 'f3a8ccdb2be2480b9acebf5913337b31',\n",
       "  'sample_token': '0b1755bf21ba4417b5a5c63cf620d36e',\n",
       "  'ego_pose_token': 'f3a8ccdb2be2480b9acebf5913337b31',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886329897423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886329897423.jpg',\n",
       "  'prev': '027121f7599a4b1aaf359b3ed787b5b6',\n",
       "  'next': '7b031059e713443b8b38b8601b8b3d8b'},\n",
       " {'token': 'e691d25718164d0482cf83f17460699e',\n",
       "  'sample_token': 'cdb3834353f14888a0ff07af5a9cf749',\n",
       "  'ego_pose_token': 'e691d25718164d0482cf83f17460699e',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886327447423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': True,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'samples/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886327447423.jpg',\n",
       "  'prev': 'fc76e72cb0c24fbc953d37cfd579cf76',\n",
       "  'next': '73be4a5ea7bf402285e67b024903e404'},\n",
       " {'token': '45ae8865a8bd4c3b8c9945ec5128fc35',\n",
       "  'sample_token': 'cb8fbf228afd41fcb6e81ae7fe18ba61',\n",
       "  'ego_pose_token': '45ae8865a8bd4c3b8c9945ec5128fc35',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886324697423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886324697423.jpg',\n",
       "  'prev': '84f78a9585a24c119201126dd4685fcd',\n",
       "  'next': '8f4a2a4bca85494788792474dbcb4947'},\n",
       " {'token': 'c7d8c42e5d534bfd94567fda2ec87044',\n",
       "  'sample_token': '672b12bead7d47d2a5cb8dc566a114a2',\n",
       "  'ego_pose_token': 'c7d8c42e5d534bfd94567fda2ec87044',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886325947423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': True,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'samples/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886325947423.jpg',\n",
       "  'prev': 'cb28854b30ef41ab8af9c5e7a0a6940c',\n",
       "  'next': 'd0c2743bb6284ff6825f21e5a76ceae2'},\n",
       " {'token': '8ebd972d24634f67bed05cb2a04ce8a5',\n",
       "  'sample_token': '672b12bead7d47d2a5cb8dc566a114a2',\n",
       "  'ego_pose_token': '8ebd972d24634f67bed05cb2a04ce8a5',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886325647423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886325647423.jpg',\n",
       "  'prev': 'c6e8f0f8425f495ba943d8624822cd7c',\n",
       "  'next': 'cf162e8304c040218baf84a57fde1cd0'},\n",
       " {'token': '5e3a87daf006462a9db497734761b021',\n",
       "  'sample_token': '2e62af9882e144f8adc3d60bbd68804b',\n",
       "  'ego_pose_token': '5e3a87daf006462a9db497734761b021',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886322897423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886322897423.jpg',\n",
       "  'prev': '4ec101e1b38d4648a5257996ec775619',\n",
       "  'next': 'ad7e2bedd23f44f0bc9997c60c6605f8'},\n",
       " {'token': 'c93d1c4074aa4757a3e80cc882cc4ee7',\n",
       "  'sample_token': '688f86c9082242edaa9f4b19d759e3e0',\n",
       "  'ego_pose_token': 'c93d1c4074aa4757a3e80cc882cc4ee7',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886323147423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886323147423.jpg',\n",
       "  'prev': '020b2fa4246149b7b72dd1eae5ac0b4b',\n",
       "  'next': 'bbd1e3c82b87480d8e37e3c5bc625bc8'},\n",
       " {'token': 'eb1f8990fa4d4b59a4431dca9858d980',\n",
       "  'sample_token': '96ee33f7aa5e48bbab4b36f7ceead005',\n",
       "  'ego_pose_token': 'eb1f8990fa4d4b59a4431dca9858d980',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886323697429,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886323697429.jpg',\n",
       "  'prev': '8577c0d7061841f9b8b0414606937eee',\n",
       "  'next': 'd9c0e9b0b678495cb2bd9e7e62654c7a'},\n",
       " {'token': 'cadf1140de4e4486922438c3a7469112',\n",
       "  'sample_token': '92f7cfd3805042238eebd1fbff915f30',\n",
       "  'ego_pose_token': 'cadf1140de4e4486922438c3a7469112',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886330547423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886330547423.jpg',\n",
       "  'prev': '8c5d83f985774f059f45fc7ec21b745f',\n",
       "  'next': 'e266c2b3f8ec48f4bf4733a363f47be2'},\n",
       " {'token': 'bd0dd457a4d247519c33c884fcfc84f9',\n",
       "  'sample_token': 'ff701e8d5743462ba9f6bc60eff70ab9',\n",
       "  'ego_pose_token': 'bd0dd457a4d247519c33c884fcfc84f9',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886325397423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886325397423.jpg',\n",
       "  'prev': 'ade41386daea4f738cc49bf94edb2e9e',\n",
       "  'next': '5b5f8a4a47204677bd0b79a1a603b765'},\n",
       " {'token': '145546a2be514532876968b3698ea12a',\n",
       "  'sample_token': '30941de4be51439f89ed7ea9a0ae0674',\n",
       "  'ego_pose_token': '145546a2be514532876968b3698ea12a',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886328147423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886328147423.jpg',\n",
       "  'prev': 'a1c61636aa4341818648a1f37014f7a9',\n",
       "  'next': '0707fd5c23514463b6e28a44a0fd8b2b'},\n",
       " {'token': '8061573fa1cd4e48b3925036d10e215c',\n",
       "  'sample_token': '868c93c6935944fab6366fcc5de80dfb',\n",
       "  'ego_pose_token': '8061573fa1cd4e48b3925036d10e215c',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886326647423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886326647423.jpg',\n",
       "  'prev': 'f080c459145f4c11b3557a5944b55a0a',\n",
       "  'next': 'e3b748a1343f4bcfb708bff882478b1d'},\n",
       " {'token': 'cccf0ad7f9e64fb295eaed8a495ca070',\n",
       "  'sample_token': '0cd37221c4724bfc8e1c369278c49d03',\n",
       "  'ego_pose_token': 'cccf0ad7f9e64fb295eaed8a495ca070',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886326297423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886326297423.jpg',\n",
       "  'prev': '34aee8afb41d40a0b96b48f114010cc2',\n",
       "  'next': 'fbfb440f7acd491cb77bdfd05f6c734f'},\n",
       " {'token': 'e56aa5dab4bb4d3e96071e75ef0c9312',\n",
       "  'sample_token': '688f86c9082242edaa9f4b19d759e3e0',\n",
       "  'ego_pose_token': 'e56aa5dab4bb4d3e96071e75ef0c9312',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886323547423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': True,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'samples/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886323547423.jpg',\n",
       "  'prev': '1ea12ac271314e6fa71461050a6a9853',\n",
       "  'next': '8577c0d7061841f9b8b0414606937eee'},\n",
       " {'token': 'd9c0e9b0b678495cb2bd9e7e62654c7a',\n",
       "  'sample_token': '96ee33f7aa5e48bbab4b36f7ceead005',\n",
       "  'ego_pose_token': 'd9c0e9b0b678495cb2bd9e7e62654c7a',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886323797423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886323797423.jpg',\n",
       "  'prev': 'eb1f8990fa4d4b59a4431dca9858d980',\n",
       "  'next': '20744e40252e4d5c849a2fbea22b4a1d'},\n",
       " {'token': 'f9892f70be94477295b1d634ae340897',\n",
       "  'sample_token': 'ba20a7743bae4d2dabc6be640c66ccc7',\n",
       "  'ego_pose_token': 'f9892f70be94477295b1d634ae340897',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886324397423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886324397423.jpg',\n",
       "  'prev': 'eb65894d06ba40eb83d28a77445bf259',\n",
       "  'next': '71887997184e4254aeba7f423ab6184e'},\n",
       " {'token': '61ae8158f8ed4b7299ee61a86acb3a66',\n",
       "  'sample_token': '803b00748c204b13a2a4cef2d017c5e4',\n",
       "  'ego_pose_token': '61ae8158f8ed4b7299ee61a86acb3a66',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886331297423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886331297423.jpg',\n",
       "  'prev': 'afc855649b4d4644ae65db15a85353e3',\n",
       "  'next': 'ee34b15f8cee40a49a3c2106a202a9ec'},\n",
       " {'token': '66ea6ba51e3e43079f7e5e705857f85e',\n",
       "  'sample_token': '43d5d31802ee447e9c25966c45e7b698',\n",
       "  'ego_pose_token': '66ea6ba51e3e43079f7e5e705857f85e',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886328797423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886328797423.jpg',\n",
       "  'prev': 'fb724e2419fd494eaa811a8a1775d7a6',\n",
       "  'next': '859d430f2d9440f796cdb4d30a302094'},\n",
       " {'token': 'd0c2743bb6284ff6825f21e5a76ceae2',\n",
       "  'sample_token': '0cd37221c4724bfc8e1c369278c49d03',\n",
       "  'ego_pose_token': 'd0c2743bb6284ff6825f21e5a76ceae2',\n",
       "  'calibrated_sensor_token': '6be823b0abef475393ad4b87c5211961',\n",
       "  'timestamp': 1531886326047423,\n",
       "  'fileformat': 'jpg',\n",
       "  'is_key_frame': False,\n",
       "  'height': 900,\n",
       "  'width': 1600,\n",
       "  'filename': 'sweeps/CAM_BACK_LEFT/n015-2018-07-18-11-50-34+0800__CAM_BACK_LEFT__1531886326047423.jpg',\n",
       "  'prev': 'c7d8c42e5d534bfd94567fda2ec87044',\n",
       "  'next': '9c88282772d94928994fa0da58a91735'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set[100000:100050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "scheduler_type = None\n",
    "\n",
    "\n",
    "if scheduler_type == \"steplr\":\n",
    "        print(1)\n",
    "elif scheduler_type is not None:\n",
    "    print(2)\n",
    "else:\n",
    "    print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = StepLR(optimizer=optimizer, step_size=None, gamma=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikostheodoridis/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "isvqa = ISVQA(\"/home/nikostheodoridis/isvqa/train_set.json\",\n",
    "              \"/home/nikostheodoridis/nuscenes/samples\",\n",
    "              \"/home/nikostheodoridis/isvqa/answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiviewViltForQuestionAnswering(6, 210, 40, 768, True, True, True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(isvqa, 4, shuffle=False)\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ -9.9990, -11.7384,  -7.4223,  ...,  -9.9704, -11.2511, -10.7394],\n",
       "        [ -9.6335, -11.7417,  -7.1838,  ..., -10.0912, -11.4883, -10.6841],\n",
       "        [ -9.4269, -10.0896,  -7.3474,  ..., -11.3587, -12.3218,  -9.9821],\n",
       "        [ -9.4646, -10.1867,  -7.3609,  ..., -11.3594, -12.3701, -10.1991]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = torch.randn(4, 1, 768)\n",
    "images = torch.randn(4, 6, 768)\n",
    "\n",
    "attn_scores = torch.randn(4, 1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.randn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_model_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        param = parameter.numel()\n",
    "        table.add_row([name, param])\n",
    "        total_params += param\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+------------+\n",
      "|                                       Modules                                       | Parameters |\n",
      "+-------------------------------------------------------------------------------------+------------+\n",
      "|                  preprocess.model.embeddings.img_position_embedding                 |   967680   |\n",
      "|                   preprocess.model.embeddings.embeddings.cls_token                  |    768     |\n",
      "|              preprocess.model.embeddings.embeddings.position_embeddings             |   111360   |\n",
      "|    preprocess.model.embeddings.embeddings.text_embeddings.word_embeddings.weight    |  23440896  |\n",
      "|  preprocess.model.embeddings.embeddings.text_embeddings.position_embeddings.weight  |   30720    |\n",
      "| preprocess.model.embeddings.embeddings.text_embeddings.token_type_embeddings.weight |    1536    |\n",
      "|       preprocess.model.embeddings.embeddings.text_embeddings.LayerNorm.weight       |    768     |\n",
      "|        preprocess.model.embeddings.embeddings.text_embeddings.LayerNorm.bias        |    768     |\n",
      "|      preprocess.model.embeddings.embeddings.patch_embeddings.projection.weight      |  2359296   |\n",
      "|       preprocess.model.embeddings.embeddings.patch_embeddings.projection.bias       |    768     |\n",
      "|         preprocess.model.embeddings.embeddings.token_type_embeddings.weight         |    1536    |\n",
      "|          preprocess.model.encoder.layer.0.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.0.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.0.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.0.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.0.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.0.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.0.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.0.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.0.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.0.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.0.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.0.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.0.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.0.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.0.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.0.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.1.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.1.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.1.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.1.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.1.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.1.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.1.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.1.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.1.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.1.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.1.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.1.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.1.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.1.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.1.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.1.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.2.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.2.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.2.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.2.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.2.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.2.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.2.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.2.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.2.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.2.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.2.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.2.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.2.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.2.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.2.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.2.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.3.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.3.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.3.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.3.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.3.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.3.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.3.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.3.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.3.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.3.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.3.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.3.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.3.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.3.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.3.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.3.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.4.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.4.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.4.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.4.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.4.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.4.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.4.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.4.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.4.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.4.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.4.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.4.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.4.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.4.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.4.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.4.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.5.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.5.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.5.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.5.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.5.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.5.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.5.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.5.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.5.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.5.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.5.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.5.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.5.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.5.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.5.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.5.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.6.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.6.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.6.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.6.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.6.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.6.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.6.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.6.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.6.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.6.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.6.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.6.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.6.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.6.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.6.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.6.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.7.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.7.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.7.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.7.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.7.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.7.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.7.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.7.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.7.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.7.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.7.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.7.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.7.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.7.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.7.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.7.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.8.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.8.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.8.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.8.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.8.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.8.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.8.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.8.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.8.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.8.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.8.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.8.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.8.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.8.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.8.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.8.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.9.attention.attention.query.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.9.attention.attention.query.bias           |    768     |\n",
      "|           preprocess.model.encoder.layer.9.attention.attention.key.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.9.attention.attention.key.bias            |    768     |\n",
      "|          preprocess.model.encoder.layer.9.attention.attention.value.weight          |   589824   |\n",
      "|           preprocess.model.encoder.layer.9.attention.attention.value.bias           |    768     |\n",
      "|            preprocess.model.encoder.layer.9.attention.output.dense.weight           |   589824   |\n",
      "|             preprocess.model.encoder.layer.9.attention.output.dense.bias            |    768     |\n",
      "|              preprocess.model.encoder.layer.9.intermediate.dense.weight             |  2359296   |\n",
      "|               preprocess.model.encoder.layer.9.intermediate.dense.bias              |    3072    |\n",
      "|                 preprocess.model.encoder.layer.9.output.dense.weight                |  2359296   |\n",
      "|                  preprocess.model.encoder.layer.9.output.dense.bias                 |    768     |\n",
      "|               preprocess.model.encoder.layer.9.layernorm_before.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.9.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.9.layernorm_after.weight               |    768     |\n",
      "|                preprocess.model.encoder.layer.9.layernorm_after.bias                |    768     |\n",
      "|          preprocess.model.encoder.layer.10.attention.attention.query.weight         |   589824   |\n",
      "|           preprocess.model.encoder.layer.10.attention.attention.query.bias          |    768     |\n",
      "|           preprocess.model.encoder.layer.10.attention.attention.key.weight          |   589824   |\n",
      "|            preprocess.model.encoder.layer.10.attention.attention.key.bias           |    768     |\n",
      "|          preprocess.model.encoder.layer.10.attention.attention.value.weight         |   589824   |\n",
      "|           preprocess.model.encoder.layer.10.attention.attention.value.bias          |    768     |\n",
      "|           preprocess.model.encoder.layer.10.attention.output.dense.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.10.attention.output.dense.bias            |    768     |\n",
      "|             preprocess.model.encoder.layer.10.intermediate.dense.weight             |  2359296   |\n",
      "|              preprocess.model.encoder.layer.10.intermediate.dense.bias              |    3072    |\n",
      "|                preprocess.model.encoder.layer.10.output.dense.weight                |  2359296   |\n",
      "|                 preprocess.model.encoder.layer.10.output.dense.bias                 |    768     |\n",
      "|              preprocess.model.encoder.layer.10.layernorm_before.weight              |    768     |\n",
      "|               preprocess.model.encoder.layer.10.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.10.layernorm_after.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.10.layernorm_after.bias               |    768     |\n",
      "|          preprocess.model.encoder.layer.11.attention.attention.query.weight         |   589824   |\n",
      "|           preprocess.model.encoder.layer.11.attention.attention.query.bias          |    768     |\n",
      "|           preprocess.model.encoder.layer.11.attention.attention.key.weight          |   589824   |\n",
      "|            preprocess.model.encoder.layer.11.attention.attention.key.bias           |    768     |\n",
      "|          preprocess.model.encoder.layer.11.attention.attention.value.weight         |   589824   |\n",
      "|           preprocess.model.encoder.layer.11.attention.attention.value.bias          |    768     |\n",
      "|           preprocess.model.encoder.layer.11.attention.output.dense.weight           |   589824   |\n",
      "|            preprocess.model.encoder.layer.11.attention.output.dense.bias            |    768     |\n",
      "|             preprocess.model.encoder.layer.11.intermediate.dense.weight             |  2359296   |\n",
      "|              preprocess.model.encoder.layer.11.intermediate.dense.bias              |    3072    |\n",
      "|                preprocess.model.encoder.layer.11.output.dense.weight                |  2359296   |\n",
      "|                 preprocess.model.encoder.layer.11.output.dense.bias                 |    768     |\n",
      "|              preprocess.model.encoder.layer.11.layernorm_before.weight              |    768     |\n",
      "|               preprocess.model.encoder.layer.11.layernorm_before.bias               |    768     |\n",
      "|               preprocess.model.encoder.layer.11.layernorm_after.weight              |    768     |\n",
      "|                preprocess.model.encoder.layer.11.layernorm_after.bias               |    768     |\n",
      "|                          preprocess.model.layernorm.weight                          |    768     |\n",
      "|                           preprocess.model.layernorm.bias                           |    768     |\n",
      "|                         preprocess.model.pooler.dense.weight                        |   589824   |\n",
      "|                          preprocess.model.pooler.dense.bias                         |    768     |\n",
      "|                               img_attn.in_proj_weight                               |  1769472   |\n",
      "|                                img_attn.in_proj_bias                                |    2304    |\n",
      "|                               img_attn.out_proj.weight                              |   589824   |\n",
      "|                                img_attn.out_proj.bias                               |    768     |\n",
      "|                        final_model.vilt.embeddings.cls_token                        |    768     |\n",
      "|                   final_model.vilt.embeddings.position_embeddings                   |   111360   |\n",
      "|          final_model.vilt.embeddings.text_embeddings.word_embeddings.weight         |  23440896  |\n",
      "|        final_model.vilt.embeddings.text_embeddings.position_embeddings.weight       |   30720    |\n",
      "|       final_model.vilt.embeddings.text_embeddings.token_type_embeddings.weight      |    1536    |\n",
      "|             final_model.vilt.embeddings.text_embeddings.LayerNorm.weight            |    768     |\n",
      "|              final_model.vilt.embeddings.text_embeddings.LayerNorm.bias             |    768     |\n",
      "|            final_model.vilt.embeddings.patch_embeddings.projection.weight           |  2359296   |\n",
      "|             final_model.vilt.embeddings.patch_embeddings.projection.bias            |    768     |\n",
      "|               final_model.vilt.embeddings.token_type_embeddings.weight              |    1536    |\n",
      "|          final_model.vilt.encoder.layer.0.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.0.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.0.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.0.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.0.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.0.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.0.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.0.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.0.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.0.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.0.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.0.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.0.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.0.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.0.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.0.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.1.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.1.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.1.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.1.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.1.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.1.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.1.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.1.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.1.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.1.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.1.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.1.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.1.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.1.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.1.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.1.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.2.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.2.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.2.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.2.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.2.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.2.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.2.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.2.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.2.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.2.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.2.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.2.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.2.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.2.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.2.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.2.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.3.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.3.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.3.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.3.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.3.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.3.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.3.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.3.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.3.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.3.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.3.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.3.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.3.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.3.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.3.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.3.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.4.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.4.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.4.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.4.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.4.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.4.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.4.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.4.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.4.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.4.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.4.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.4.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.4.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.4.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.4.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.4.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.5.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.5.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.5.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.5.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.5.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.5.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.5.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.5.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.5.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.5.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.5.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.5.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.5.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.5.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.5.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.5.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.6.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.6.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.6.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.6.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.6.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.6.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.6.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.6.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.6.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.6.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.6.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.6.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.6.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.6.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.6.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.6.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.7.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.7.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.7.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.7.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.7.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.7.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.7.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.7.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.7.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.7.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.7.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.7.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.7.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.7.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.7.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.7.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.8.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.8.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.8.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.8.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.8.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.8.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.8.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.8.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.8.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.8.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.8.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.8.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.8.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.8.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.8.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.8.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.9.attention.attention.query.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.9.attention.attention.query.bias           |    768     |\n",
      "|           final_model.vilt.encoder.layer.9.attention.attention.key.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.9.attention.attention.key.bias            |    768     |\n",
      "|          final_model.vilt.encoder.layer.9.attention.attention.value.weight          |   589824   |\n",
      "|           final_model.vilt.encoder.layer.9.attention.attention.value.bias           |    768     |\n",
      "|            final_model.vilt.encoder.layer.9.attention.output.dense.weight           |   589824   |\n",
      "|             final_model.vilt.encoder.layer.9.attention.output.dense.bias            |    768     |\n",
      "|              final_model.vilt.encoder.layer.9.intermediate.dense.weight             |  2359296   |\n",
      "|               final_model.vilt.encoder.layer.9.intermediate.dense.bias              |    3072    |\n",
      "|                 final_model.vilt.encoder.layer.9.output.dense.weight                |  2359296   |\n",
      "|                  final_model.vilt.encoder.layer.9.output.dense.bias                 |    768     |\n",
      "|               final_model.vilt.encoder.layer.9.layernorm_before.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.9.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.9.layernorm_after.weight               |    768     |\n",
      "|                final_model.vilt.encoder.layer.9.layernorm_after.bias                |    768     |\n",
      "|          final_model.vilt.encoder.layer.10.attention.attention.query.weight         |   589824   |\n",
      "|           final_model.vilt.encoder.layer.10.attention.attention.query.bias          |    768     |\n",
      "|           final_model.vilt.encoder.layer.10.attention.attention.key.weight          |   589824   |\n",
      "|            final_model.vilt.encoder.layer.10.attention.attention.key.bias           |    768     |\n",
      "|          final_model.vilt.encoder.layer.10.attention.attention.value.weight         |   589824   |\n",
      "|           final_model.vilt.encoder.layer.10.attention.attention.value.bias          |    768     |\n",
      "|           final_model.vilt.encoder.layer.10.attention.output.dense.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.10.attention.output.dense.bias            |    768     |\n",
      "|             final_model.vilt.encoder.layer.10.intermediate.dense.weight             |  2359296   |\n",
      "|              final_model.vilt.encoder.layer.10.intermediate.dense.bias              |    3072    |\n",
      "|                final_model.vilt.encoder.layer.10.output.dense.weight                |  2359296   |\n",
      "|                 final_model.vilt.encoder.layer.10.output.dense.bias                 |    768     |\n",
      "|              final_model.vilt.encoder.layer.10.layernorm_before.weight              |    768     |\n",
      "|               final_model.vilt.encoder.layer.10.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.10.layernorm_after.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.10.layernorm_after.bias               |    768     |\n",
      "|          final_model.vilt.encoder.layer.11.attention.attention.query.weight         |   589824   |\n",
      "|           final_model.vilt.encoder.layer.11.attention.attention.query.bias          |    768     |\n",
      "|           final_model.vilt.encoder.layer.11.attention.attention.key.weight          |   589824   |\n",
      "|            final_model.vilt.encoder.layer.11.attention.attention.key.bias           |    768     |\n",
      "|          final_model.vilt.encoder.layer.11.attention.attention.value.weight         |   589824   |\n",
      "|           final_model.vilt.encoder.layer.11.attention.attention.value.bias          |    768     |\n",
      "|           final_model.vilt.encoder.layer.11.attention.output.dense.weight           |   589824   |\n",
      "|            final_model.vilt.encoder.layer.11.attention.output.dense.bias            |    768     |\n",
      "|             final_model.vilt.encoder.layer.11.intermediate.dense.weight             |  2359296   |\n",
      "|              final_model.vilt.encoder.layer.11.intermediate.dense.bias              |    3072    |\n",
      "|                final_model.vilt.encoder.layer.11.output.dense.weight                |  2359296   |\n",
      "|                 final_model.vilt.encoder.layer.11.output.dense.bias                 |    768     |\n",
      "|              final_model.vilt.encoder.layer.11.layernorm_before.weight              |    768     |\n",
      "|               final_model.vilt.encoder.layer.11.layernorm_before.bias               |    768     |\n",
      "|               final_model.vilt.encoder.layer.11.layernorm_after.weight              |    768     |\n",
      "|                final_model.vilt.encoder.layer.11.layernorm_after.bias               |    768     |\n",
      "|                          final_model.vilt.layernorm.weight                          |    768     |\n",
      "|                           final_model.vilt.layernorm.bias                           |    768     |\n",
      "|                         final_model.vilt.pooler.dense.weight                        |   589824   |\n",
      "|                          final_model.vilt.pooler.dense.bias                         |    768     |\n",
      "|                           final_model.classifier.0.weight                           |  1179648   |\n",
      "|                            final_model.classifier.0.bias                            |    1536    |\n",
      "|                           final_model.classifier.1.weight                           |    1536    |\n",
      "|                            final_model.classifier.1.bias                            |    1536    |\n",
      "|                           final_model.classifier.3.weight                           |  4806144   |\n",
      "|                            final_model.classifier.3.bias                            |    3129    |\n",
      "+-------------------------------------------------------------------------------------+------------+\n",
      "Total Trainable Params: 232513593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "232513593"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(isvqa, 1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.Size' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1913, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = torch.randn(1, 768)\n",
    "images = torch.randn(6, 768)\n",
    "\n",
    "img_attn = nn.MultiheadAttention(768, 12)\n",
    "\n",
    "_, attn_scores = img_attn(questions, images, images)\n",
    "\n",
    "attn_scores[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_step(model, loader, acc_fn, answ_len):\n",
    "    \"\"\"\n",
    "    A function that validates the model by going through all the mini-batches in the validation dataloader once.\n",
    "    \"\"\"\n",
    "    print(\"\\tValidating...\")\n",
    "    model.eval()\n",
    "    losses = []  # to save the loss of each mini-batch in order to take their average at the end\n",
    "    accuracies = []  # to save the accuracy of each mini-batch in order to take their average at the end\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i, (X, y) in enumerate(loader):\n",
    "            outputs = model(**X, labels=y)\n",
    "            loss = outputs.loss\n",
    "            pred = max_to_one_hot(outputs.logits)\n",
    "            acc = acc_fn(pred, y, answ_len)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            accuracies.append(acc)\n",
    "\n",
    "    avg_loss = sum(losses) / len(loader)\n",
    "    avg_acc = sum(accuracies) / len(loader)\n",
    "    \n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikostheodoridis/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = MultiviewViltForQuestionAnswering(6, 210, 768, True, False, False).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.classifier = nn.Sequential(\n",
    "        nn.Linear(768, 1536),\n",
    "        nn.LayerNorm(1536),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(1536, 429)\n",
    "    ).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.load_state_dict(torch.load(\"/home/nikostheodoridis/Trained Models/2024-07-08 00:07:49/model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p1, p2 in zip(model.parameters(), trained_model.parameters()):\n",
    "#     assert torch.equal(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set = ISVQA(qa_path=\"/home/nikostheodoridis/isvqa/val_set.json\",\n",
    "                nuscenes_path=\"/home/nikostheodoridis/nuscenes/samples\",\n",
    "                answers_path=\"/home/nikostheodoridis/isvqa/answers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_set, batch_size=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets = []\n",
    "# untrained_predictions = []\n",
    "# trained_predictions = []\n",
    "\n",
    "# model.eval()\n",
    "# trained_model.eval()\n",
    "# for i in range(2576):\n",
    "#     inputs, target = val_set[i]\n",
    "\n",
    "#     targets.append(target)\n",
    "\n",
    "#     with torch.inference_mode():\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions: torch.Tensor, targets: torch.Tensor, answers_len: int) -> float:\n",
    "    cnt = torch.eq(torch.eq(predictions, targets).sum(dim=1), answers_len).sum()\n",
    "    return cnt.item() / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidating...\n"
     ]
    }
   ],
   "source": [
    "untrained_loss, untrained_acc = val_step(model, val_loader, accuracy, 429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tValidating...\n"
     ]
    }
   ],
   "source": [
    "trained_loss, trained_acc = val_step(trained_model, val_loader, accuracy, 429)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303.8099614342978\n",
      "0.0011627906976744186\n"
     ]
    }
   ],
   "source": [
    "print(untrained_loss)\n",
    "print(untrained_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5099486532945967\n",
      "0.6003875968992246\n"
     ]
    }
   ],
   "source": [
    "print(trained_loss)\n",
    "print(trained_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/nikostheodoridis/isvqa/answers_counter.json\") as f:\n",
    "    answers_cnt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'yes': 13564,\n",
       "         'no': 3734,\n",
       "         'one': 3663,\n",
       "         'white': 2893,\n",
       "         'two': 2544,\n",
       "         'red': 1205,\n",
       "         'black': 1046,\n",
       "         'blue': 1025,\n",
       "         'three': 986,\n",
       "         'green': 968,\n",
       "         'yellow': 930,\n",
       "         'orange': 782,\n",
       "         'four': 529,\n",
       "         'night': 464,\n",
       "         'rainy': 434,\n",
       "         'gray': 365,\n",
       "         'black and white': 345,\n",
       "         'silver': 287,\n",
       "         'zero': 254,\n",
       "         'five': 218,\n",
       "         'orange and white': 178,\n",
       "         'six': 156,\n",
       "         'left': 151,\n",
       "         'none': 147,\n",
       "         'ahead': 147,\n",
       "         'right': 141,\n",
       "         'fedex': 136,\n",
       "         'brown': 134,\n",
       "         'cloudy': 129,\n",
       "         'slow': 119,\n",
       "         'ups': 114,\n",
       "         'bus': 112,\n",
       "         'raining': 111,\n",
       "         'wet': 111,\n",
       "         'sunny': 107,\n",
       "         'ryder': 95,\n",
       "         'urban': 93,\n",
       "         'twenty-three': 92,\n",
       "         'stop': 89,\n",
       "         'day': 77,\n",
       "         'hump': 69,\n",
       "         'brick': 69,\n",
       "         'red and white': 57,\n",
       "         'twenty-five': 55,\n",
       "         'bridge': 55,\n",
       "         'rectangle': 54,\n",
       "         'trees': 53,\n",
       "         'truck': 52,\n",
       "         'city': 51,\n",
       "         'toyota': 50,\n",
       "         'nineteen': 48,\n",
       "         'on': 46,\n",
       "         'rain': 40,\n",
       "         'seven': 39,\n",
       "         'forty': 37,\n",
       "         'french': 36,\n",
       "         'black and yellow': 36,\n",
       "         'innosparks': 35,\n",
       "         'square': 34,\n",
       "         'ford': 34,\n",
       "         'blue and green': 34,\n",
       "         'umbrella': 33,\n",
       "         'night time': 33,\n",
       "         'metal': 32,\n",
       "         'arrow': 32,\n",
       "         'van': 31,\n",
       "         'motorcycle': 29,\n",
       "         'parked': 29,\n",
       "         'fence': 29,\n",
       "         'construction': 28,\n",
       "         'nighttime': 28,\n",
       "         'p': 28,\n",
       "         'eversource energy': 28,\n",
       "         'green and blue': 28,\n",
       "         'nuss': 27,\n",
       "         'car': 27,\n",
       "         'crosswalk': 27,\n",
       "         'boston': 26,\n",
       "         'dark': 26,\n",
       "         'one way': 26,\n",
       "         'cars': 25,\n",
       "         'tree': 25,\n",
       "         'yankee': 25,\n",
       "         'do not enter': 25,\n",
       "         'water': 25,\n",
       "         'building': 25,\n",
       "         'twelve': 24,\n",
       "         'twenty-one': 24,\n",
       "         'pink': 24,\n",
       "         'suv': 23,\n",
       "         'starbucks': 23,\n",
       "         'north coast': 23,\n",
       "         'bicycle': 23,\n",
       "         'blue and white': 23,\n",
       "         'john nagle': 23,\n",
       "         'hump ahead': 22,\n",
       "         'eight': 22,\n",
       "         'behind': 22,\n",
       "         'bike': 21,\n",
       "         'xfinity': 21,\n",
       "         'sidewalk': 21,\n",
       "         'm': 20,\n",
       "         'harbor place': 20,\n",
       "         'usps': 20,\n",
       "         'both': 20,\n",
       "         'round': 20,\n",
       "         'parking lot': 20,\n",
       "         'white and orange': 19,\n",
       "         'orange and gray': 19,\n",
       "         'only': 19,\n",
       "         'stop sign': 19,\n",
       "         'ninety': 19,\n",
       "         'chain link': 19,\n",
       "         'penske': 19,\n",
       "         'fusionopolis two': 19,\n",
       "         'jeep': 18,\n",
       "         'holland ave': 18,\n",
       "         'gemalto': 18,\n",
       "         'white and yellow': 18,\n",
       "         'concrete': 18,\n",
       "         'hood': 18,\n",
       "         'circle': 17,\n",
       "         'gray and orange': 17,\n",
       "         'solid': 17,\n",
       "         'river': 17,\n",
       "         'synthesis': 17,\n",
       "         'backpack': 17,\n",
       "         'nine': 16,\n",
       "         'row thirty-four': 16,\n",
       "         'grass': 16,\n",
       "         'sportello': 16,\n",
       "         'yellow and black': 16,\n",
       "         'starbucks coffee': 16,\n",
       "         'dhl': 16,\n",
       "         'residential': 16,\n",
       "         'v': 16,\n",
       "         'caution': 15,\n",
       "         'one north gateway': 15,\n",
       "         'back right': 15,\n",
       "         'dog': 15,\n",
       "         'clouds': 15,\n",
       "         'crane': 15,\n",
       "         'nucleos': 15,\n",
       "         'bus stop': 14,\n",
       "         'closed': 14,\n",
       "         'nine hundred and seventy': 14,\n",
       "         'fire hydrant': 14,\n",
       "         'one north link': 14,\n",
       "         'open': 14,\n",
       "         'harvey': 13,\n",
       "         'glass': 13,\n",
       "         'bricks': 13,\n",
       "         'worldwide services': 13,\n",
       "         'speedpost': 12,\n",
       "         'walking': 12,\n",
       "         'road': 12,\n",
       "         'wheelchair': 12,\n",
       "         'museum': 12,\n",
       "         'seventy-five': 12,\n",
       "         'more': 12,\n",
       "         'innovationsquareboston': 12,\n",
       "         'yankee lobster': 12,\n",
       "         'sedan': 11,\n",
       "         'rectangular': 11,\n",
       "         'yellow and white': 11,\n",
       "         'x ing': 11,\n",
       "         'harpoon brewery': 11,\n",
       "         'evening': 11,\n",
       "         'canteen': 11,\n",
       "         'business link': 11,\n",
       "         'down': 11,\n",
       "         'ladder': 10,\n",
       "         'front': 10,\n",
       "         'ten': 10,\n",
       "         'wood': 10,\n",
       "         'msc': 10,\n",
       "         'overcast': 10,\n",
       "         'barbed wire': 10,\n",
       "         'two way': 10,\n",
       "         'shorts': 10,\n",
       "         'woman': 10,\n",
       "         'cat': 9,\n",
       "         'three way': 9,\n",
       "         'sysco': 9,\n",
       "         'centros': 9,\n",
       "         'light': 9,\n",
       "         'parking garage': 9,\n",
       "         'x': 9,\n",
       "         'empty': 9,\n",
       "         'works ahead': 9,\n",
       "         'ninety-three': 9,\n",
       "         'mercedes': 9,\n",
       "         'diamond': 9,\n",
       "         'biopolis dr': 9,\n",
       "         'front left': 9,\n",
       "         'cones': 9,\n",
       "         'commercial': 9,\n",
       "         'dunkin donuts': 9,\n",
       "         'seafood': 9,\n",
       "         'suffolk': 9,\n",
       "         's': 9,\n",
       "         'skanska': 9,\n",
       "         '': 8,\n",
       "         'mass bay credit union': 8,\n",
       "         'one hundred and fifty-one': 8,\n",
       "         'triangle': 8,\n",
       "         'four hundred and fifty-one': 8,\n",
       "         'five eleven': 8,\n",
       "         'macco energy': 8,\n",
       "         'mail truck': 8,\n",
       "         'back': 8,\n",
       "         'honda': 8,\n",
       "         'helmet': 8,\n",
       "         'nus': 8,\n",
       "         'double': 8,\n",
       "         'industrial': 8,\n",
       "         'squares': 8,\n",
       "         'bag': 8,\n",
       "         'taxi': 8,\n",
       "         'stopped': 7,\n",
       "         'four way': 7,\n",
       "         'whiskey priest': 7,\n",
       "         'traffic lights': 7,\n",
       "         'go': 7,\n",
       "         'morning': 7,\n",
       "         'usa': 7,\n",
       "         'reduce speed now': 7,\n",
       "         'dry': 7,\n",
       "         'green and white': 7,\n",
       "         'gold': 7,\n",
       "         'bicycles': 7,\n",
       "         'we deliver for you': 7,\n",
       "         'eleven': 7,\n",
       "         'bench': 7,\n",
       "         'innovation and design': 7,\n",
       "         'legal': 7,\n",
       "         'la casa de pedro': 7,\n",
       "         'sandcrawler': 7,\n",
       "         'traffic cones': 7,\n",
       "         'fort point market': 7,\n",
       "         'exit only': 7,\n",
       "         'red and gray': 7,\n",
       "         'vpne': 7,\n",
       "         'autodesk': 6,\n",
       "         'scooter': 6,\n",
       "         'rounded': 6,\n",
       "         'brown and white': 6,\n",
       "         'stars ave': 6,\n",
       "         'female': 6,\n",
       "         'enterprise': 6,\n",
       "         'twenty-two': 6,\n",
       "         'orange and black': 6,\n",
       "         'forward': 6,\n",
       "         'white and black': 6,\n",
       "         'male': 6,\n",
       "         \"don't walk\": 6,\n",
       "         'bmw': 6,\n",
       "         'red light': 6,\n",
       "         'blue harvest': 6,\n",
       "         'plants': 6,\n",
       "         'to right': 6,\n",
       "         'dashed': 6,\n",
       "         'red and blue': 6,\n",
       "         'available liftgate service': 6,\n",
       "         'www': 6,\n",
       "         'away': 6,\n",
       "         'boston freight terminals': 6,\n",
       "         'parking': 6,\n",
       "         'flett': 6,\n",
       "         'rosa mexicano': 5,\n",
       "         'on sidewalk': 5,\n",
       "         'back left': 5,\n",
       "         'cone': 5,\n",
       "         'isuzu': 5,\n",
       "         'windows': 5,\n",
       "         'sbwtc': 5,\n",
       "         'lights': 5,\n",
       "         'american': 5,\n",
       "         'portsdown rd': 5,\n",
       "         'e': 5,\n",
       "         'man': 5,\n",
       "         'true': 5,\n",
       "         'dark blue': 5,\n",
       "         'flowers': 5,\n",
       "         'arch': 5,\n",
       "         'wagamama': 5,\n",
       "         'fifteen': 5,\n",
       "         'curved': 5,\n",
       "         'kent ridge drive': 5,\n",
       "         'd': 5,\n",
       "         '4x4': 5,\n",
       "         'clear': 5,\n",
       "         'railroad crossing': 5,\n",
       "         'moving': 5,\n",
       "         'ayer rajah': 5,\n",
       "         'sitting': 5,\n",
       "         'phone': 5,\n",
       "         'salmon': 5,\n",
       "         'parking meter': 5,\n",
       "         'person': 5,\n",
       "         'straight': 5,\n",
       "         'xpo': 5,\n",
       "         'supercamp': 5,\n",
       "         'minivan': 5,\n",
       "         'tcoms': 5,\n",
       "         'factory outlet': 5,\n",
       "         'english': 5,\n",
       "         'palm tree': 5,\n",
       "         'mediapolis': 5,\n",
       "         'sign': 5,\n",
       "         'audi': 5,\n",
       "         'thirty-six': 5,\n",
       "         'seaport world trade center': 5,\n",
       "         'mail': 5,\n",
       "         'all': 5,\n",
       "         'construction ahead': 4,\n",
       "         'suburban': 4,\n",
       "         'mitsubishi': 4,\n",
       "         'bush': 4,\n",
       "         'sunglasses': 4,\n",
       "         'yang ming': 4,\n",
       "         'blue and yellow': 4,\n",
       "         'mitsubishi electric': 4,\n",
       "         'double decker bus': 4,\n",
       "         'towards': 4,\n",
       "         'up': 4,\n",
       "         'legal harborside': 4,\n",
       "         'lobster': 4,\n",
       "         'fios': 4,\n",
       "         'radius bank': 4,\n",
       "         'standing': 4,\n",
       "         'arrows': 4,\n",
       "         'triangles': 4,\n",
       "         'lexus': 4,\n",
       "         'green and red': 4,\n",
       "         'false': 4,\n",
       "         'nothing': 4,\n",
       "         'pedestrians': 4,\n",
       "         'gateway': 4,\n",
       "         'cranes': 4,\n",
       "         'paul w': 4,\n",
       "         'harpoon': 4,\n",
       "         'smoke shop': 4,\n",
       "         'red and green': 4,\n",
       "         'donuts': 4,\n",
       "         'green and orange': 4,\n",
       "         'jeans': 4,\n",
       "         'blue and orange': 4,\n",
       "         'singapore science park': 4,\n",
       "         'ship': 4,\n",
       "         'alumni house': 4,\n",
       "         'on road': 4,\n",
       "         'stroller': 4,\n",
       "         'exit': 4,\n",
       "         'bushes': 4,\n",
       "         'detour': 4,\n",
       "         'blue harvest fisheries': 4,\n",
       "         'partly cloudy': 4,\n",
       "         'w': 4,\n",
       "         'bank of america': 4,\n",
       "         'white car': 4,\n",
       "         'cell phone': 4,\n",
       "         'innovation': 4,\n",
       "         'mountain': 4,\n",
       "         'excavator': 3,\n",
       "         'yield': 3,\n",
       "         'three hundred': 3,\n",
       "         'thirty-five': 3,\n",
       "         'mazda': 3,\n",
       "         'dusk': 3,\n",
       "         'sun': 3,\n",
       "         'beige': 3,\n",
       "         'crossing street': 3,\n",
       "         'singapore': 3,\n",
       "         'outside': 3,\n",
       "         'bad': 3,\n",
       "         'site access': 3,\n",
       "         'dead end': 3,\n",
       "         'geico': 3,\n",
       "         'four hundred and eleven': 3,\n",
       "         'stairs': 3,\n",
       "         'low': 3,\n",
       "         'h': 3,\n",
       "         'sahara': 3,\n",
       "         'chevy': 3,\n",
       "         'buildings': 3,\n",
       "         'sbs transit': 3,\n",
       "         'six hundred and seventeen five hundred and twenty-three eight thousand': 3,\n",
       "         'fusionopolis': 3,\n",
       "         'traffic light': 3,\n",
       "         'daylight': 3,\n",
       "         'self storage': 3,\n",
       "         'palm trees': 3,\n",
       "         'american flag': 3,\n",
       "         'short': 3,\n",
       "         'seventy-nine': 3,\n",
       "         'biopolis rd': 3,\n",
       "         'street': 3,\n",
       "         'mandm': 3,\n",
       "         'blue dragon': 3,\n",
       "         'humps': 3,\n",
       "         'twenty': 3,\n",
       "         'white and red': 3,\n",
       "         't': 3,\n",
       "         'mailbox': 3,\n",
       "         'peapod': 3,\n",
       "         'congress st': 3,\n",
       "         'future starts here': 3,\n",
       "         'park': 3,\n",
       "         'large': 3,\n",
       "         'construction workers': 3,\n",
       "         'mirror': 3,\n",
       "         'hat': 3,\n",
       "         'twenty-eight': 3,\n",
       "         'foley': 3,\n",
       "         '02148m0003': 3,\n",
       "         'tan': 3,\n",
       "         'synapse': 3,\n",
       "         'business': 3,\n",
       "         'green and yellow': 3,\n",
       "         'pickup truck': 3,\n",
       "         'pandg': 3,\n",
       "         'dark gray': 3,\n",
       "         'intersection': 3,\n",
       "         'public parking': 3,\n",
       "         'for lease': 3,\n",
       "         'striped': 3,\n",
       "         'light blue': 3,\n",
       "         'trucks': 3})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(answers_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Start\n",
    "import json\n",
    "import random\n",
    "train_path = \"/home/nikostheodoridis/nuscenes-qa/train_set.json\"\n",
    "\n",
    "val_path = \"/home/nikostheodoridis/nuscenes-qa/val_set.json\"\n",
    "\n",
    "test_path = \"/home/nikostheodoridis/nuscenes-qa/test_set.json\"\n",
    "with open(train_path) as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(val_path) as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "with open(test_path) as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "for data in train_data:\n",
    "    if data in test_data:\n",
    "        print(\"False\")\n",
    "        break\n",
    "else:\n",
    "    print(\"True\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
